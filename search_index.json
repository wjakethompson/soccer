[
["index.html", "Soccer Predictions Using Bayesian Mixed Effects Models Preface", " Soccer Predictions Using Bayesian Mixed Effects Models Jake Thompson 2017-03-30 Preface This document was created in partial fulfillment of the requirements for the comprehensive examination for the Educational Psychology and Research doctoral program at the University of Kansas. The task assigned was to create a rating system for European soccer teams, and to use these ratings to predict the winners of the major domestic leagues (the German Budesliga, the Spanish La Liga, the French Ligue 1, the English Premier League, and the Italian Serie A), and the winner of the Union of European Football Associations (UEFA) Champion’s League. Each section of this document describes a specific step in the process of creating these predictions, from gathering the necessary data to creating output graphics. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["intro.html", "1 Introduction 1.1 European soccer format 1.2 Document organization 1.3 Colophon", " 1 Introduction The goal of this project is to predict winners of the major domestic European soccer leagues and the UEFA Champions League. Necessarily, this means that the real goal is to accurately predict the outcomes of individual games. To do this, I estimate team abilities using Bayesian mixed effects models. The estimated abilities can then be used to predict the outcomes of games that have yet to be played. Before diving into the process of developing and estimating these models, it will be useful to have a brief introduction to how competitions are structured for European soccer teams. An understanding of the various competitions will help clarify why teams are or are not included in the estimation of the model, and make more obvious how predictions should be made. 1.1 European soccer format Unlike most American sports leagues (e.g., NFL, NBA, MLB, etc.), European soccer teams compete in a variety of domestic and international competitions. In general, these competitions fall into one of three major categories: Domestic leagues Domestic cups FIFA/UEFA Competitions 1.1.1 Domestic leagues Domestic leagues are most analogous to the regular season of the major American sports. In most domestic European soccer leagues, the league follows a double round robin structure with each team playing every other team twice: once at home and once away. Teams are awarded 3 points for a victory, 1 point for a tie, and 0 points for a loss. Once all the games have been played, the team with the most points wins the league. 1.1.2 Domestic cups European leagues also host domestic cups. For the most part, domestic tournaments consist of all eligible professional teams from a country, not just the teams from the highest league (this would be analogous to a baseball tournament where minor league and major league teams all competed against each other). These cups can either be straight knockout style tournaments (e.g., March Madness for NCAA basketball), or each leg can have two legs where the teams involved play twice, once at each team’s home stadium. Because of the vast number of teams involved in these cups, entrance into the tournament is usually staggered so that the best teams are guaranteed places in the later rounds. 1.1.3 FIFA and UEFA Competitions These are the international tournaments. For this project, the focus is on the UEFA Champions League. This tournament consists of several qualifying rounds before the tournament proper (e.g., play-in games). Once the tournament proper begins, the final 32 teams are placed into 8 groups of 4 teams each. A team plays every team in its group twice, once at home and once away, with points awarded the same as for domestic leagues (see section 1.1.1). At the end of group play, the two teams with the most points from each group advance to the final 16. From this point on, the tournament follows the structure of a knockout tournament, with each round having two legs (one home and one away game). This continues until the final, which is only a single game played at a neutral location. 1.2 Document organization This document steps through the process that was followed to create the predictions for the major European domestic leagues and the UEFA Champions League. I start by defining two possible models for estimating club ability in Section 2. I then conduct a small scale simulation study in Section 3 to determine if one of the models is better able to recover estimates of team ability. Sections 4 and 5 focus on gather the data to be used and estimating the model respectively. In Section 6, the estimates from the model are used to predict the outcomes of the domestic leagues and the UEFA Champions League, and output graphics are created. Finally, Section 7 outlines the limitations of this approach and possible improvements to the model. 1.3 Colophon The source code for this document can be found at https://github.com/wjakethompson/soccer. The document was written with bookdown (Xie, 2016a), which simplifies the process of turning multiple R markdown files into a single output file (e.g., HTML, PDF, EPUB). This document was built with: load(&quot;_data/session_info.rda&quot;) devtools:::print.session_info(session_info) #&gt; Session info -------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.3.3 (2017-03-06) #&gt; system x86_64, darwin13.4.0 #&gt; ui RStudio (1.0.136) #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/Chicago #&gt; date 2017-03-28 #&gt; Packages ------------------------------------------------------------------ #&gt; package * version date #&gt; assertthat 0.1 2013-12-06 #&gt; backports 1.0.4 2016-10-24 #&gt; base64enc 0.1-3 2015-07-28 #&gt; BH 1.62.0-1 2016-11-19 #&gt; bitops 1.0-6 2013-08-17 #&gt; bookdown 0.3.7 2017-01-15 #&gt; car 2.1-3 2016-08-11 #&gt; caTools 1.17.1 2014-09-10 #&gt; colorspace 1.2-6 2015-03-11 #&gt; curl 2.1 2016-09-22 #&gt; DBI 0.5-1 2016-09-10 #&gt; dichromat 2.0-0 2013-01-24 #&gt; digest 0.6.12 2017-01-27 #&gt; dplyr * 0.5.0 2016-06-24 #&gt; DT 0.2 2016-08-09 #&gt; evaluate 0.10 2016-10-11 #&gt; GGally 1.3.0 2017-03-05 #&gt; ggplot2 * 2.2.1.9000 2017-03-27 #&gt; gridExtra 2.2.1 2016-02-29 #&gt; gtable 0.2.0 2016-02-26 #&gt; highr 0.6 2016-05-09 #&gt; htmltools 0.3.5 2016-03-21 #&gt; htmlwidgets 0.7 2016-08-02 #&gt; httr 1.2.1 2016-07-03 #&gt; inline 0.3.14 2015-04-13 #&gt; jsonlite 1.2 2016-12-31 #&gt; knitr 1.15.1 2016-11-22 #&gt; labeling 0.3 2014-08-23 #&gt; lattice 0.20-34 2016-09-06 #&gt; lazyeval 0.2.0.9000 2016-09-19 #&gt; lme4 1.1-12 2016-04-16 #&gt; lubridate * 1.6.0.9009 2017-03-27 #&gt; magrittr 1.5 2014-11-22 #&gt; markdown 0.7.7 2015-04-22 #&gt; MASS 7.3-45 2016-04-21 #&gt; Matrix 1.2-8 2017-01-20 #&gt; MatrixModels 0.4-1 2015-08-22 #&gt; mgcv 1.8-17 2017-02-08 #&gt; mime 0.5 2016-07-07 #&gt; minqa 1.2.4 2014-10-09 #&gt; munsell 0.4.3 2016-02-13 #&gt; nlme 3.1-131 2017-02-06 #&gt; nloptr 1.0.4 2014-08-04 #&gt; nnet 7.3-12 2016-02-02 #&gt; openssl 0.9.4 2016-05-25 #&gt; pbkrtest 0.4-6 2016-01-27 #&gt; plyr 1.8.4.9000 2016-11-03 #&gt; portableParallelSeeds * 0.97 2016-11-14 #&gt; prettyunits 1.0.2 2015-07-13 #&gt; progress 1.1.2 2016-12-14 #&gt; purrr * 0.2.2.9000 2017-03-27 #&gt; quantreg 5.29 2016-09-04 #&gt; R6 2.2.0 2016-10-05 #&gt; RColorBrewer 1.1-2 2014-12-07 #&gt; Rcpp 0.12.10 2017-03-27 #&gt; RcppEigen 0.3.2.9.0 2016-08-21 #&gt; reshape 0.8.6 2016-10-21 #&gt; reshape2 1.4.2 2016-10-22 #&gt; rlang 0.0.0.9004 2017-03-27 #&gt; rmarkdown 1.3 2016-12-21 #&gt; rockchalk 1.8.101 2016-02-25 #&gt; rprojroot 1.1 2016-10-29 #&gt; rstan * 2.14.1 2016-12-28 #&gt; rvest * 0.3.2 2016-06-17 #&gt; scales 0.4.1 2016-11-09 #&gt; selectr 0.3-0 2016-08-30 #&gt; SparseM 1.72 2016-09-06 #&gt; StanHeaders * 2.14.0-1 2017-01-09 #&gt; stringi 1.1.3 2017-03-21 #&gt; stringr 1.2.0 2017-02-18 #&gt; tibble 1.2-15 2017-03-05 #&gt; xml2 * 1.0.0 2016-06-24 #&gt; yaml 2.1.14 2016-11-12 #&gt; source #&gt; CRAN (R 3.3.0) #&gt; cran (@1.0.4) #&gt; cran (@0.1-3) #&gt; cran (@1.62.0-) #&gt; CRAN (R 3.3.0) #&gt; Github (rstudio/bookdown@2211cd0) #&gt; CRAN (R 3.3.0) #&gt; cran (@1.17.1) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; cran (@0.5-1) #&gt; CRAN (R 3.3.0) #&gt; cran (@0.6.12) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; Github (ggobi/GGally@4ed1c3b) #&gt; Github (hadley/ggplot2@f4398b6) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; cran (@0.6) #&gt; cran (@0.3.5) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; cran (@1.2) #&gt; cran (@1.15.1) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.3) #&gt; Github (hadley/lazyeval@c155c3d) #&gt; CRAN (R 3.3.0) #&gt; Github (hadley/lubridate@82dd6ba) #&gt; CRAN (R 3.3.0) #&gt; cran (@0.7.7) #&gt; CRAN (R 3.3.3) #&gt; CRAN (R 3.3.3) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.3) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.3) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.3) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; Github (hadley/plyr@fe19241) #&gt; CRAN (R 3.3.1) #&gt; cran (@1.0.2) #&gt; cran (@1.1.2) #&gt; Github (hadley/purrr@55d4dbe) #&gt; CRAN (R 3.3.0) #&gt; cran (@2.2.0) #&gt; CRAN (R 3.3.0) #&gt; Github (RcppCore/Rcpp@876d635) #&gt; CRAN (R 3.3.0) #&gt; cran (@0.8.6) #&gt; cran (@1.4.2) #&gt; Github (hadley/rlang@fbfc06c) #&gt; cran (@1.3) #&gt; CRAN (R 3.3.0) #&gt; cran (@1.1) #&gt; CRAN (R 3.3.2) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.2) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.0) #&gt; CRAN (R 3.3.2) #&gt; cran (@1.1.3) #&gt; cran (@1.2.0) #&gt; Github (hadley/tibble@4c32758) #&gt; cran (@1.0.0) #&gt; cran (@2.1.14) References "],
["define-model.html", "2 Defining the Model 2.1 The bivariate Poisson model 2.2 The game random intercept model", " 2 Defining the Model In soccer, the goals scored by a single team can be thought of as coming from a Poisson distribution. Thus, we can state that the two scores from a given game, \\(X\\) and \\(Y\\), are Poisson distributed. \\[\\begin{equation} \\begin{split} X &amp; \\sim Poisson(\\lambda_1)\\\\ Y &amp; \\sim Poisson(\\lambda_2) \\end{split} \\tag{2.1} \\end{equation}\\] This parameterization, however, assumes that the scores \\(X\\) and \\(Y\\) are independent of each other. In this project, the aim is to model the association between the two Poisson distributed variables. There are two methods for modeling this association that will be examined. The first is a bivariate Poisson distribution. The second is a mixed effects model with a random slope for each game. 2.1 The bivariate Poisson model In the bivariate Poisson, the scores for teams \\(A\\) and \\(B\\), \\(X_A\\) and \\(X_B\\), are random variables where \\(G_i \\sim Poisson(\\lambda_i),\\ i = 0,\\ 1,\\ 2\\). \\[\\begin{equation} \\begin{split} X_A &amp; = G_1 + G_0\\\\ X_B &amp; = G_2 + G_0 \\end{split} \\tag{2.2} \\end{equation}\\] And \\(X_A\\) and \\(X_B\\) are jointly distributed \\[\\begin{equation} (X_A,\\ X_B) \\sim BP(\\lambda_1,\\ \\lambda_2,\\ \\lambda_0) \\tag{2.3} \\end{equation}\\] In this parameterization, \\(X_A\\) and \\(X_B\\) are Poisson distributed with means equal to \\(\\lambda_1 + \\lambda_3\\) and \\(\\lambda_2 + \\lambda_3\\) respectively, with \\(\\lambda_3\\) representing the covariance between \\(X_A\\) and \\(X_B\\) (AlMuhayfith, Alzaid, &amp; Omair, 2016; Griffiths &amp; Milne, 1978; Kawamura, 1973). We can model these parameters just as we would model, for example, means in a normal distribution. Thus, for a given game, \\(i\\), \\[\\begin{equation} \\begin{split} (X_{Ai},\\ X_{Bi}) &amp; \\sim BP(\\lambda_{1i},\\ \\lambda_{2i},\\ \\lambda_{0i}),\\\\ log(\\lambda_{1i}) &amp; = \\omega_{1i}\\beta_{1},\\\\ log(\\lambda_{2i}) &amp; = \\omega_{2i}\\beta_{2},\\\\ log(\\lambda_{0i}) &amp; = \\omega_{0i}\\beta_{0} \\end{split} \\tag{2.4} \\end{equation}\\] where \\(\\omega\\) represents a matrix of independent variable, and \\(\\beta\\) denotes the regression coefficients (Karlis &amp; Ntzoufras, 2003, 2005). When predicting soccer games, the independent variables are the teams that are playing, and the regression coefficients represent the offensive or defensive strength of the teams (Groll, Kneib, Mayr, &amp; Schauberger, 2016). Specifically, we can model the two scores, \\(X\\), for teams \\(A\\) and \\(B\\), from a given game, \\(i\\), as \\[\\begin{align} log(X_{Ai}) &amp;= \\lambda_{1i} + \\lambda_{0i}, \\notag \\\\ log(X_{Bi}) &amp;= \\lambda_{2i} + \\lambda_{0i}, \\notag \\\\ \\lambda_{1i} &amp;= \\mu + \\eta H_i + \\alpha_A + \\delta_B, \\tag{2.5} \\\\ \\lambda_{2i} &amp;= \\mu + \\alpha_B + \\delta_A, \\tag{2.6} \\\\ \\lambda_{0i} &amp;= \\rho_A + \\rho_B \\tag{2.7} \\end{align}\\] Here, \\(\\mu\\) denotes the overall intercept, or the expected log goals for a team not playing at home, and \\(\\eta\\) represents the increase in expected log goals for a team playing at home. \\(H_i\\) is a dummy variable indicating whether game \\(i\\) was played at the home team’s stadium (1) or a neutral site (0). The estimates of team ability come from \\(\\alpha\\) and \\(\\delta\\), which represent the attacking and defensive abilities of the given team respectively. These can be modeled as fixed effects, or as random effects. Finally, \\(\\rho\\) denotes the change in expected covariance for each team (Whitaker, 2011). 2.1.1 Implementing the bivariate Poisson model The bivariate Poisson model can be fit using the following Stan code and the rstan package (Guo, Gabry, &amp; Goodrich, 2017). In the model code, I have modeled \\(\\alpha\\), \\(\\delta\\), and \\(\\rho\\) as random effects so that \\(\\mu\\) represents the overall mean. This means that positive \\(\\alpha\\) values and negative \\(\\delta\\) are good, as team wants the attack to add goals above the average, and the defense to result in the opponent have below average goals. All parameters have non-information priors. The priors were specified as normal with a mean of 0 and standard deviation of 10. This specification provides a diffuse range of plausible values for the parameter, allowing the likelihood to dominate. However, the use of a diffuse normal prior also prevents arbitrary boundaries from being set for a uniformly distributed prior. I have also reparameterized the random effects so that Stan can sample from a \\(\\mathcal{N}(0,\\ 1)\\), which reduces computation time and increases the efficiency of the sampler to avoid divergent transitions (Betancourt, 2016, 2017; Stan Development Team, 2016c). data { int&lt;lower=1&gt; num_clubs; // number of clubs int&lt;lower=1&gt; num_games; // number of games int&lt;lower=1,upper=num_clubs&gt; home[num_games]; // home club for game g int&lt;lower=1,upper=num_clubs&gt; away[num_games]; // away club for game g int&lt;lower=0&gt; h_goals[num_games]; // home goals for game g int&lt;lower=0&gt; a_goals[num_games]; // away goals for game g int&lt;lower=0,upper=1&gt; homeg[num_games]; // home field for game g } parameters { vector[num_clubs] raw_alpha; // attacking intercepts vector[num_clubs] raw_delta; // defending intercepts vector[num_clubs] raw_rho; // covariance intercepts real mu; // fixed intercept real eta; // homefield real&lt;lower=0&gt; sigma_a; // attacking sd real&lt;lower=0&gt; sigma_d; // defending sd real&lt;lower=0&gt; sigma_r; // covariance sd } transformed parameters { vector[num_clubs] alpha; vector[num_clubs] delta; vector[num_clubs] rho; alpha = raw_alpha * sigma_a; delta = raw_delta * sigma_d; rho = raw_rho * sigma_r; } model { vector[num_games] lambda1; vector[num_games] lambda2; vector[num_games] lambda3; // priors raw_alpha ~ normal(0, 1); raw_delta ~ normal(0, 1); raw_rho ~ normal(0, 1); mu ~ normal(0, 10); eta ~ normal(0, 10); sigma_a ~ normal(0, 10); sigma_d ~ normal(0, 10); sigma_r ~ normal(0, 10); // likelihood for (g in 1:num_games) { lambda1[g] = exp(mu + (eta * homeg[g]) + alpha[home[g]] + delta[away[g]]); lambda2[g] = exp(mu + alpha[away[g]] + delta[home[g]]); lambda3[g] = exp(rho[home[g]] + rho[away[g]]); } h_goals ~ poisson(lambda1 + lambda3); a_goals ~ poisson(lambda2 + lambda3); } 2.2 The game random intercept model As an alternative to the bivariate Poisson model, one could model a random intercept for each game, rather than estimating \\(\\rho\\). Thus, the game random intercept model would be defined as \\[\\begin{align} log(X_{Ai}) &amp;= \\lambda_{1i}, \\notag \\\\ log(X_{Bi}) &amp;= \\lambda_{2i}, \\notag \\\\ \\lambda_{1i} &amp;= \\mu + \\eta H_i + \\alpha_A + \\delta_B + \\gamma_i, \\tag{2.8} \\\\ \\lambda_{2i} &amp;= \\mu + \\alpha_B + \\delta_A + \\gamma_i \\tag{2.9} \\end{align}\\] This model is very similar to the bivariate Poisson. The two rate parameters, \\(\\lambda_{1i}\\) and \\(\\lambda_{2i}\\), are defined the same, with only the addition of \\(\\gamma_i\\) denoting the random intercept for the game. This \\(\\gamma_i\\) replaces \\(\\lambda_{0i}\\) in the bivariate Poisson model. This has a couple of downstream effects on the estimation. First, in the bivariate Poisson model, \\(\\rho\\) is estimated for each team. Thus the convariance, \\(\\lambda_{0i}\\) is predicted for each game by the competing teams’ \\(\\rho\\) values. This also allows predictions to be made for future games about what the covariance or dependency between the teams will be. In contrast, game random intercept model doesn’t estimate predictors for this dependency. In this model, the dependency is treated as a random variable, with some variance to be estimated. Thus, although both models take into account the dependency between the two scores in a given game, the models make different assumptions about the nature of this dependency. 2.2.1 Implementing the game random intercept model The game random intercept model can be estimated using the following Stan code and the rstan package (Guo et al., 2017). As with the bivariate Poisson model, I have modeled \\(\\alpha\\) and \\(\\delta\\) as random effects so that \\(\\mu\\) represents the overall mean. Diffuse normal priors are specified in the same way as for the bivariate Poisson model. The random effects are also reparameterized in the same way as they were in the bivariate Poisson to reduce computation time and increase efficiency (Betancourt, 2016, 2017; Stan Development Team, 2016c). data { int&lt;lower=1&gt; num_clubs; // number of clubs int&lt;lower=1&gt; num_games; // number of games int&lt;lower=1,upper=num_clubs&gt; home[num_games]; // home club for game g int&lt;lower=1,upper=num_clubs&gt; away[num_games]; // away club for game g int&lt;lower=0&gt; h_goals[num_games]; // home goals for game g int&lt;lower=0&gt; a_goals[num_games]; // away goals for game g int&lt;lower=0,upper=1&gt; homeg[num_games]; // home field for game g } parameters { vector[num_clubs] raw_alpha; // attacking intercepts vector[num_clubs] raw_delta; // defending intercepts vector[num_games] raw_gamma; // game intercepts real mu; // fixed intercept real eta; // homefield real&lt;lower=0&gt; sigma_a; // attacking sd real&lt;lower=0&gt; sigma_d; // defending sd real&lt;lower=0&gt; sigma_g; // game sd } transformed parameters { vector[num_clubs] alpha; vector[num_clubs] delta; vector[num_games] gamma; alpha = sigma_a * raw_alpha; delta = sigma_d * raw_delta; gamma = sigma_g * raw_gamma; } model { vector[num_games] lambda1; vector[num_games] lambda2; // priors raw_alpha ~ normal(0, 1); // attacking random effects raw_delta ~ normal(0, 1); // defending random effects raw_gamma ~ normal(0, 1); // game random effects mu ~ normal(0, 10); eta ~ normal(0, 10); sigma_a ~ normal(0, 10); sigma_d ~ normal(0, 10); sigma_g ~ normal(0, 10); // likelihood for (g in 1:num_games) { lambda1[g] = exp(mu + (eta * homeg[g]) + alpha[home[g]] + delta[away[g]] + gamma[g]); lambda2[g] = exp(mu + alpha[away[g]] + delta[home[g]] + gamma[g]); } h_goals ~ poisson(lambda1); a_goals ~ poisson(lambda2); } References "],
["simulation.html", "3 Simulation Study 3.1 Data generation 3.2 Model estimation 3.3 Running the simulation 3.4 Simulation results 3.5 Summary of findings", " 3 Simulation Study In order to evaluate these two models, I conducted a small scale simulation study. I generated 100 data sets from each of the bivariate Poisson and game random intercept models (for 200 data sets total). For each data set, both of the models were estimated to determine how well they were able to recover the attacking and defensive parameters for each team when the data generating model did and did not match the model. 3.1 Data generation Data was simulated to mimic the major domestic European leagues. For each data set, 20 clubs were generated, with each club playing all other clubs twice, once at home, and once away, for a total of 380 games. For both data generating models, \\(\\alpha\\) and \\(\\delta\\) parameters were drawn from a \\(\\mathcal{N}(0,0.35)\\). Both \\(\\rho\\) in the bivariate Poisson and \\(\\gamma\\) in the game random intercept model were sampled from a \\(\\mathcal{N}(0,0.1)\\) distribution. For all models, \\(\\mu\\) was set to 0, and \\(\\eta\\) was set to 0.5. These distributions were based on preliminary analyses using the 2015-16 English Premier League data. For each game, \\(\\lambda\\) values were calculated based on the parameters that were generated for each team (and game for the game random intercept model). Scores were then randomly generated using the rpois() function. Full data generation functions can be see in Appendix A.1 and A.2. When simulating the data sets, the portableParallelSeeds package was used to ensure that the data generation was completely replicable, and that the random number streams were not overlapping (P. E. Johnson, 2016). 3.2 Model estimation In total, 200 data sets were generated: 100 from the bivariate Poisson, and 100 from the game random intercept model. For each data set, both the bivariate Poisson and game random intercept model were estimated. Thus, each model was estimated 100 times on a data set from the matching data generation method, and 100 times on mismatched data. For each estimation, 2 chains were run with 15000 iterations. The first 5000 iterations of each chain were discarded for burn-in. This resulted in a total of 20000 retained iterations that made up the final posterior distributions. The thinning interval was set to 1 (no thinning). Finally, the target proposal acceptance rate during the adaptation period was set to 0.99. This forces the algorithm to take smaller steps, which decreases efficiency. However, this also helps to eliminate divergent transitions, which are common when parameter estimates are close to their bounds, such as a variance very close to 0 (Stan Development Team, 2016c, 2016a). The full model estimation function can be seen in Appendix A.3. 3.3 Running the simulation The following code was used to run the simulation. I first define the total number of replications and the number of random streams needed for each replication. I then use portableParallelSeeds (P. E. Johnson, 2016) to create seeds for each replication, and save them. Data sets are then generated from each model, with the number of data sets from each model being equal to half of the total number of replications specified by n_reps. Finally, I create one list that contains all of the data sets, simulation_data, and map the simulation function to each element of that list using the purrr package (Wickham, 2016b). # Define parameters for the simulation n_reps &lt;- 200 streams_per_rep &lt;- 1 # Create the seed warehouse project_seeds &lt;- seedCreator(n_reps, streams_per_rep, seed = 9416) save(project_seeds, file = &quot;_data/simulation_seeds.rda&quot;) # Create data sets bivpois_data &lt;- lapply(X = 1:(n_reps / 2), FUN = generate_bivpois, seeds = project_seeds, num_club = 20) gri_data &lt;- lapply(X = ((n_reps / 2) + 1):n_reps, FUN = generate_gri, seeds = project_seeds, num_club = 20) simulation_data &lt;- c( bivpois_data, gri_data ) simulation &lt;- map_df(.x = simulation_data, .f = simulation_fun) save(simulation, file = &quot;_data/simulation.rda&quot;) 3.4 Simulation results 3.4.1 Correlation between true and estimated parameters To assess the bivariate Poisson and game random intercept models, I first examine the correlations between the true parameters and the estimated parameters from each model. The parameters for each replication can be pulled out of the simulation results using the purrr and dplyr packages (Wickham, 2016b; Wickham &amp; Francois, 2016). The plyr package will be used later but, for compatibility reasons, needs to be loaded before dplyr. library(plyr) library(dplyr) library(purrr) plot_sim &lt;- simulation %&gt;% select(generator, true_params, bivpois_params, gri_params) %&gt;% as.list() %&gt;% pmap_df(.l = ., .f = function(generator, true_params, bivpois_params, gri_params) { data_frame( generator = generator, true_alpha = true_params$attack, true_delta = true_params$defend, bivpois_alpha = bivpois_params$bivpois_alpha, bivpois_delta = bivpois_params$bivpois_delta, gri_alpha = gri_params$gri_alpha, gri_delta = gri_params$gri_delta ) }) I then use the GGally package (Schloerke et al., 2016), an extension of ggplot2 (Wickham &amp; Chang, 2016) to plot a scatter plot matrix with correlations. I use the round_any() function from the plyr package (Wickham, 2016a) to set the limits on the axes. The functions to create the lower and upper triangles and the diagonal of the scatter plot matrix (lowerFn(), upperFn(), and diagFn() respectively) can be found in Appendix A.4.1, and the code to put the plots together can be found in Appendix A.4.2. Figure 3.1: Correlation between true and estimated alpha parameters under different generating and estimated models. Figure 3.2: Correlation between true and estimated delta parameters under different generating and estimated models. In the Figures 3.1 and 3.2, the margins show which model was estimated. The upper triangle shows the correlations between the estimated parameters for the model and the true parameters under each of the data generation conditions. For example, when estimating the alpha parameters with the bivariate Poisson (Figure 3.1), the correlation between estimated parameters and true parameters is 0.831 when the data was generated with the GRI model and 0.832 when generated with the bivariate Poisson model. When looking at Figures 3.1 and 3.2, the correlation between true and estimated parameters is higher when the game random intercept model is used for estimation compared to the bivariate Poisson. This is true when the game random intercept model is used to generate data (0.907 to 0.831 for alpha and 0.903 to 0.821 for delta) and when the bivariate Poisson model is use for data generation (0.847 to 0.832 for alpha and 0.861 to 0.849 for delta). Thus, this provides preliminary evidence that the game random intercept model should be preferred. 3.4.2 Estimation bias and mean square error It is also useful to look at the average bias and mean squared error of the estimates from each model. Table 3.1 shows the average bias in the estimates for alpha and delta (calculated as \\(estimate - true\\)). simulation %&gt;% select(`Data Generator` = generator, bivpois_alpha_bias, bivpois_delta_bias, gri_alpha_bias, gri_delta_bias) %&gt;% group_by(`Data Generator`) %&gt;% summarize( `Bivariate Poisson: Alpha` = sprintf(&quot;%.3f&quot;, mean(bivpois_alpha_bias)), `Bivariate Poisson: Delta` = sprintf(&quot;%.3f&quot;, mean(bivpois_delta_bias)), `Game Random Intercept: Alpha` = sprintf(&quot;%.3f&quot;, mean(gri_alpha_bias)), `Game Random Intercept: Delta` = sprintf(&quot;%.3f&quot;, mean(gri_delta_bias)) ) %&gt;% mutate(`Data Generator` = factor(`Data Generator`, levels = c(&quot;bivpois&quot;, &quot;gri&quot;), labels = c(&quot;Bivariate Poisson&quot;, &quot;Game Random Intercept&quot;))) %&gt;% knitr::kable(caption = &quot;Estimation bias for alpha and delta parameters.&quot;) Table 3.1: Estimation bias for alpha and delta parameters. Data Generator Bivariate Poisson: Alpha Bivariate Poisson: Delta Game Random Intercept: Alpha Game Random Intercept: Delta Bivariate Poisson 0.001 -0.012 0.001 -0.012 Game Random Intercept -0.010 -0.003 -0.010 -0.002 The bias is nearly identical across estimation models, regardless of which model was used for data generation. Further, all of the biases are very close to 0, indicating that neither model significantly biases the estimates in a positive or negative way. Table 3.2 shows the average mean square error in the estimates for alpha and delta (calculated as \\((estimate - true)^2\\)). simulation %&gt;% select(`Data Generator` = generator, bivpois_alpha_mse, bivpois_delta_mse, gri_alpha_mse, gri_delta_mse) %&gt;% group_by(`Data Generator`) %&gt;% summarize( `Bivariate Poisson: Alpha` = sprintf(&quot;%.3f&quot;, mean(bivpois_alpha_mse)), `Bivariate Poisson: Delta` = sprintf(&quot;%.3f&quot;, mean(bivpois_delta_mse)), `Game Random Intercept: Alpha` = sprintf(&quot;%.3f&quot;, mean(gri_alpha_mse)), `Game Random Intercept: Delta` = sprintf(&quot;%.3f&quot;, mean(gri_delta_mse)) ) %&gt;% mutate(`Data Generator` = factor(`Data Generator`, levels = c(&quot;bivpois&quot;, &quot;gri&quot;), labels = c(&quot;Bivariate Poisson&quot;, &quot;Game Random Intercept&quot;))) %&gt;% knitr::kable(caption = &quot;Estimation mean square error for alpha and delta parameters.&quot;) Table 3.2: Estimation mean square error for alpha and delta parameters. Data Generator Bivariate Poisson: Alpha Bivariate Poisson: Delta Game Random Intercept: Alpha Game Random Intercept: Delta Bivariate Poisson 0.038 0.037 0.046 0.049 Game Random Intercept 0.083 0.069 0.022 0.022 As would be expected, there is more error associated with estimates coming from a model that doesn’t match the data generating model. However, comparatively, there is less error associated with the game random intercept model. When the estimation model matches the generation model, the game random intercept model has mean square error values of 0.022 and 0.022 for alpha and delta parameters respectively. The bivariate Poisson model has mean square error values of 0.038 and 0.037 for these same parameters. Similarly, the game random intercept model also shows lower error when the estimation model doesn’t match the generation model. The game random intercept model has mean square error values for alpha and delta of 0.046 and 0.049 when the bivariate Poisson generated the data, whereas the bivariate Poisson has mean square error values for alpha and delta of 0.083 and 0.069 when the game random intercept model generated the data. These results indicate that the best choice of model would be the one that matches the data generation process. Unfortunately, when using real data, the exact data generation process is impossible to know. However, these findings also show that the cost of being wrong is much less when using the game random intercept model to estimate, compared to the bivariate Poisson model. 3.5 Summary of findings The findings from the simulation study indicate that the game random intercept model should be the preferred model for estimating the abilities of soccer teams. Across data generating models, the game random intercept model was better able to recover the true parameters (Section 3.4.1). Additionally, the game random intercept model had comparable bias and lower mean square error than the the bivariate Poisson model (Section 3.4.2). Therefore, I will be using the game random intercept model moving forward for estimating the model on real data and predicting future games. References "],
["gather-data.html", "4 Gather Data 4.1 Domestic league inclusion 4.2 Non-domestic competition inclusion 4.3 Domestic competition inclusion 4.4 Collect club websites 4.5 Scrape game data", " 4 Gather Data Perhaps the most important part of any data analysis is the collection of the data. If you don’t have the data necessary to support the model you want to estimate, or the conclusions you want to draw, no amount of tinkering with the model or results can save you. The data for this project was collected using web scraping via the rvest (Wickham, 2016c) and dplyr (Wickham &amp; Francois, 2016) packages. The scores for each game in a season for a given team can be found on ESPN’s website. For example, all of Barcelona’s games can be found here. Given that there is access to the results for any team available on ESPN, the question becomes which teams to include in the analysis. I chose teams that particpated in specific domestic leagues or in certain tournaments or competitions. The rationale for each selection is below. 4.1 Domestic league inclusion Because one goal of the project was to predict the 5 major European domestic leagues, all teams from the English Premier League, German Bundesliga, French Ligue 1, Spanish La Liga, and Italian Serie A were included. I also included the second tier leagues from these countries, if they were available. This included the English League Championship, German 2. Bundesliga, French Ligue 2, and Italian Serie B. The other major goal was to predict the UEFA Champion’s League (UCL). Many of the UCL teams come from the major domestic leagues, however many teams do not. Thus, I also included teams from the Belgian Jupiler League, Danish SAS-Ligaen, Dutch Eredivisie, Portuguese Liga, Russian Premier League, Scottish Premiership, Swiss Super League, and Turkish Super Lig. These leagues were selected because they all had teams reach the knockout stage of the UCL, and thus would have sufficient crossover with the major leagues. To get the URLs for each team, I’ll write a function that uses the rvest package to pull hyperlinks of of the league pages called scrape_league(). This function takes in a URL for a given league, and returns a data frame with the club names, the totals goals scored by and against each club in league play, the points each club has accumulated, and the club’s URL. See Appendix B.1 for the full function. For a full description of how the rvest package works, see this webinar (Grolemund, 2016). 4.2 Non-domestic competition inclusion Although many of the teams participating in the UCL come from the leagues the leagues outlines above, not all due. A few come from smaller leagues that are not covered by ESPN. Thus, all teams that participated in the group stage of the UCL were included. Additionally, teams that didn’t qualify for the UCL, and some teams that did qualify for the UCL, but didn’t make it out of the group stage, play in the second tier Eurpoa League tournament. Because this tournament includes more overlap between the leagues, all teams participating in the Europa league were also included. Finally, the International Champions Cup is a relatively new event that pairs top teams from Europe against each other around the world. All participants in this event were also included. The webpages for these competitions are formatted slightly differently than those for the domestic leagues, so I will need a slightly different function to scrape these team URLs, scrape_major_cup() (see Appendix B.2). Unlike the league scraper, this function only returns the club, and the URL for the club. 4.3 Domestic competition inclusion Finally, in addition to domestic leagues and international club competitions, European teams also compete in domestic tournaments. These competitions were also included to increase the number of data points for the top teams, as well as to increase the overlap between the first and second tier leagues of the top countries. All participants were included from Spain’s Copa del Ray and Spanish Super Cup, Italy’s Coppa Italia, France’s Coupe de France and Coupe de la Ligue, Germany’s DFB Pokal, and England’s FA Cup and League Cup. These webpages are also formatted differently, so I have one final function to get the team URLs, scrape_dom_cup() (see Appendix B.3). As with the scraper for the international competitions, this scraper also returns the club name and club URL. 4.4 Collect club websites Now that criteria for which teams will be included has been defined, the purrr package (Wickham, 2016b) can be used to map the each function the corresponding league or competition URL. For more information on the purrr package see R for Data Science (Wickham &amp; Grolemund, 2016). First I’ll create a list of URLs for each type of scraper. leagues &lt;- list( belgium = &quot;http://www.espnfc.us/belgian-jupiler-league/6/table&quot;, denmark = &quot;http://www.espnfc.us/danish-sas-ligaen/7/table&quot;, england = &quot;http://www.espnfc.us/english-premier-league/23/table&quot;, england2 = &quot;http://www.espnfc.us/english-league-championship/24/table&quot;, france = &quot;http://www.espnfc.us/french-ligue-1/9/table&quot;, france2 = &quot;http://www.espnfc.us/french-ligue-2/96/table&quot;, germany = &quot;http://www.espnfc.us/german-bundesliga/10/table&quot;, germany2 = &quot;http://www.espnfc.us/german-2-bundesliga/97/table&quot;, italy = &quot;http://www.espnfc.us/italian-serie-a/12/table&quot;, italy2 = &quot;http://www.espnfc.us/italian-serie-b/99/table&quot;, netherlands = &quot;http://www.espnfc.us/dutch-eredivisie/11/table&quot;, portugal = &quot;http://www.espnfc.us/portuguese-liga/14/table&quot;, russia = &quot;http://www.espnfc.us/russian-premier-league/106/table&quot;, scotland = &quot;http://www.espnfc.us/scottish-premiership/45/table&quot;, spain = &quot;http://www.espnfc.us/spanish-primera-division/15/table&quot;, switzerland = &quot;http://www.espnfc.us/swiss-super-league/17/table&quot;, turkey = &quot;http://www.espnfc.us/turkish-super-lig/18/table&quot; ) major_cups &lt;- list( champions_league = &quot;http://www.espnfc.us/uefa-champions-league/2/table&quot;, europa_league = &quot;http://www.espnfc.us/uefa-europa-league/2310/table&quot;, icc = &quot;http://www.espnfc.us/international-champions-cup/2326/table&quot; ) domestic_cups &lt;- list( copa_del_rey = &quot;http://www.espnfc.us/spanish-copa-del-rey/80/statistics/fairplay&quot;, coppa_italia = &quot;http://www.espnfc.us/italian-coppa-italia/2192/statistics/fairplay&quot;, coupe_de_france = &quot;http://www.espnfc.us/french-coupe-de-france/182/statistics/fairplay&quot;, coupe_de_la_ligue = &quot;http://www.espnfc.us/french-coupe-de-la-ligue/159/statistics/fairplay&quot;, dfb_pokal = &quot;http://www.espnfc.us/german-dfb-pokal/2061/statistics/fairplay&quot;, efl_cup = &quot;http://www.espnfc.us/efl-cup/41/statistics/fairplay&quot;, fa_cup = &quot;http://www.espnfc.us/english-fa-cup/40/statistics/fairplay&quot;, spanish_super_cup = &quot;http://www.espnfc.us/spanish-super-cup/431/statistics/fairplay&quot; ) Then I can then map the functions to the defined URLs. Because there are many teams to scrape, it’s possible to overload ESPN’s server request limit. Therefore, I’ve define a safe version of the read_html() function that will allow us to check if the website was read correctly. library(rvest) library(purrr) library(dplyr) safe_read_html &lt;- safely(read_html) league_urls &lt;- map_df(.x = leagues, .f = scrape_league) major_cup_urls &lt;- map_df(.x = major_cups, .f = scrape_major_cup) domestic_cup_urls &lt;- map_df(.x = domestic_cups, .f = scrape_dom_cup) The next step is to create a data frame of all the clubs and their URLs. Many teams participate in multiple competitions that were included, so only one instance will be kept. url_lookup &lt;- list( select(league_urls, club, club_url), major_cup_urls, domestic_cup_urls ) %&gt;% bind_rows() %&gt;% filter(!is.na(club_url)) %&gt;% unique() full_urls &lt;- league_urls %&gt;% select(-club_url) %&gt;% full_join(url_lookup, by = &quot;club&quot;) %&gt;% filter(!is.na(club_url)) DT::datatable(full_urls, options = list(pageLength = 5, scrollX = TRUE), rownames = FALSE) 4.5 Scrape game data Now that all of the team URLs have been collected, I can scrape the game data from each team’s page on ESPN. To do this, I’ll define a new scraper function, scrape_team() (see Appendix B.4). Then this function gets mapped to all of the club URLs, using the same process that was used for the league URLs. Note that I also use the lubridate package (Grolemund, Spinu, &amp; Wickham, 2016) to format dates within the scrape_team() function. library(lubridate) full_data &lt;- map2_df(.x = full_urls$club_url, .y = full_urls$club, .f = scrape_team) Finally, there is just little cleaning to be done. I will remove duplicate games, and replace the abbreviations that ESPN uses in their scores with the full club name. team_lookup &lt;- select(full_data, -team_data) full_data &lt;- bind_rows(full_data$team_data) %&gt;% unique() %&gt;% arrange(date, home) %&gt;% left_join(select(full_data, -team_data), by = c(&quot;home&quot; = &quot;abbrev&quot;)) %&gt;% rename(home_club = club) %&gt;% left_join(select(full_data, -team_data), by = c(&quot;away&quot; = &quot;abbrev&quot;)) %&gt;% rename(away_club = club) %&gt;% mutate( real_home = ifelse(is.na(home_club), home, home_club), real_away = ifelse(is.na(away_club), away, away_club), home = real_home, away = real_away ) %&gt;% select(-(home_club:real_away)) %&gt;% mutate(home_game = ifelse(competition %in% c(&quot;Champions Cup&quot;), 0, 1)) %&gt;% filter(!(date &lt; Sys.Date() &amp; is.na(home_goals))) %&gt;% filter(date &gt; ymd(&quot;2016-03-01&quot;)) %&gt;% rename(h_goals = home_goals, a_goals = away_goals) DT::datatable(full_data, options = list(pageLength = 5, scrollX = TRUE)) References "],
["fit-model.html", "5 Fitting the Model 5.1 Model estimation 5.2 MCMC diagnostics 5.3 Model fit 5.4 Results", " 5 Fitting the Model With the data gathered, the next step is to estimate the model to get ability estimates for each team. Following the findings of the simulation study in Section 3, the game random intercept model will be used for the estimation. 5.1 Model estimation The first step is to load in the data set and do some filtering using the dplyr package (Wickham &amp; Francois, 2016). library(dplyr) load(&quot;_data/full_data.rda&quot;) To filter, I first remove any game that hasn’t been played (or was canceled) by removing any games that don’t have a score for either team. I also remove the competition field, as it is not necessary for the model estimation. fit_data &lt;- full_data %&gt;% filter(!is.na(h_goals), !is.na(a_goals)) %&gt;% select(-competition) Next, I filter the data to only include teams with a least 5 games played. This is to ensure that all teams included have a sufficient number of games to get reasonable estimates. This process is done iteratively to ensure that after removing teams with less than five games, teams that orginally had more than 5 games, but now no longer meet the criteria are also excluded. filter_games &lt;- TRUE while(filter_games) { team_counts &lt;- table(c(fit_data$home, fit_data$away)) %&gt;% as_data_frame() %&gt;% arrange(desc(n)) %&gt;% filter(n &gt;= 5) %&gt;% select(team = Var1, games = n) %&gt;% arrange(team) %&gt;% mutate(code = seq_len(nrow(.))) fit_data &lt;- fit_data %&gt;% left_join(select(team_counts, -games), by = c(&quot;home&quot; = &quot;team&quot;)) %&gt;% rename(home_code = code) %&gt;% left_join(select(team_counts, -games), by = c(&quot;away&quot; = &quot;team&quot;)) %&gt;% rename(away_code = code) %&gt;% filter(!is.na(home_code), !is.na(away_code)) new_min &lt;- table(c(fit_data$home, fit_data$away)) %&gt;% as.numeric() %&gt;% min() if (new_min &gt;= 5) { filter_games &lt;- FALSE } else { fit_data &lt;- fit_data %&gt;% select(-home_code, -away_code) } } This filtering process leaves a total of 6608 games between 432 teams. The number of games for each team ranges from 5 to 55. To estimate the model, the data has to be supplied to Stan in a list format. The names of the elements of the list should match the names of the input data specified in the Stan model (see Section 2.2.1 for Stan definition of the game random intercept model). stan_data &lt;- list( num_clubs = nrow(team_counts), num_games = nrow(fit_data), home = fit_data$home_code, away = fit_data$away_code, h_goals = fit_data$h_goals, a_goals = fit_data$a_goals, homeg = fit_data$home_game ) In order to later assess model fit with posterior predictive model checks, a generated quantities section was added to the Stan model specification. generated quantities { int home_rep[num_games]; int away_rep[num_games]; for (g in 1:num_games) { home_rep[g] = poisson_rng(exp(mu + (eta * homeg[g]) + alpha[home[g]] + delta[away[g]] + gamma[g])); away_rep[g] = poisson_rng(exp(mu + alpha[away[g]] + delta[home[g]] + gamma[g])); } } This section attempt to replicated the original data. At each each iteration of the chain, the \\(\\lambda\\) for each score within each game is calculated using the current values of the parameters. A random Poisson is then drawn for each team using the calculated \\(\\lambda\\) values. Thus, the model is able to produce a posterior distribution for the scores of each game. These distribution can then be compared to the observed scores to see how well the model fits the data. Finally, the model can be estimated using the Stan interface rstan (Guo et al., 2017). The model is estimated with 3 chains, each with 4000 iterations. The first 2000 iterations from each chain are discarded as burn in. Additionally, a thinning interval of 2 was used. These decisions were made to decrease computation time, and to limit the size of the result stanfit objects, which become quite large. This leaves 1000 iterations from each chain, for a total of 3000 iterations that will make up the final posterior distributions. As in the simulation study, the target proposal acceptance rate during the adaptation period was set to 0.99 (see Section 3.2; Stan Development Team, 2016c, 2016a). library(rstan) gri_stanfit &lt;- stan(file = &quot;_data/stan-models/gri_ppmc.stan&quot;, data = stan_data, chains = 3, iter = 7000, warmup = 2000, init = &quot;random&quot;, thin = 5, cores = 3, algorithm = &quot;NUTS&quot;, seed = 9416, control = list(adapt_delta = 0.99, max_treedepth = 15)) 5.2 MCMC diagnostics After estimating the model, before the parameters can be analyzed and inferences can be made, the model has to be checked to make sure it estimated correctly and completely. This diagnostic information is critical to Markov Chain Monte Carlo estimation, as with proper estimation, no valid inferences can be made. The code used to create the plot and tables used to display the diagnostic information can be seen in Appendix C. 5.2.1 Convergence The first check is convergence. Convergence means that the MCMC chain found the high density area of the posterior distribution, and stayed there. When multiple chains are estimated, this can be done by verifying that the estimates from each chain end up in the same place. For a single chain, this means verifying that the chain is sampling roughly the same area at the beginning of the chain (after burn in) as in the end of the chain. One way to assess convergence is through the \\(\\hat{R}\\) statistic (Brooks &amp; Gelman, 1997; A. Gelman et al., 2014). The \\(\\hat{R}\\) statistic is also known as the potential scale reduction, and is a measure of how much variance there is between chains compared to the amount of variation within chains. Gelman and Rubin (1992) suggest that in order to conclude the model has successfully converged, all \\(\\hat{R}\\) values should be less than 1.1. Figure 5.1: Rhat statistics for the estimated game random intercept model. Figure 5.1 shows that all of the \\(\\hat{R}\\) values are below the suggested cutoff of 1.1, indicating that the model has converged. 5.2.2 Efficiency The second important check for MCMC estimation is the efficiency of the sampler. In other words, it is important to check that the algorithm adequately sampled the full posterior. There are several ways this can be examined. The first is by examing the effective sample size. This diagnostic takes into account the autocorrelation in the chain to determine the ‘effective’ number of independent draws from the posterior. If the chain is slow moving, the draws will be highly autocorrelated, and effective sample size will be well below the total number of iterations in the chain. However, low autocorrelations would indicate that the sampler is moving around the posterior fairly quickly, and the effictive sample size will be at or near the true sample size of the posterior. The effective sample size for all parameters can been seen in Figure 5.2. Figure 5.2: Effective sample size for the estimated game random intercept model parameters. There are also measure of efficiency that are exclusive the No U-Turn Sampler (NUTS; Hoffman &amp; Gelman, 2014). For example the Bayesian Factor of Missing Information (BFMI) gives an estimate of how well the sampler adpated and explored the posterior distribution. The BFMI ranges from 0 to 1, with 0 and 1 representing poor and excellent estimation respectively. This is calculated for the chain overall (Betancourt, 2016). Table 5.1: Diagnostic statistics for the NUTS algorithm. Chain BFMI Mean Acceptance Rate Max Treedepth Chain 1 0.966 0.991 7 Chain 2 0.844 0.982 6 Chain 3 0.890 0.982 6 The BFMI values in Table 5.1 indicate that the sampler was able to adequately visit the posterior distributions. Additionally, Table 5.1 shows the mean acceptance rate for each chain. As expected, these values are very close to the 0.99 that was specified when the model was estimated (control = list(adapt_delta = 0.99); Section 5.1). As noted in Sections 3.2 and 5.1, a target acceptance rate this high is needed to prevent divergent transitions. This occurs due to the small variances of \\(\\alpha\\), \\(\\delta\\), and \\(\\gamma\\) that are estimated. The high target acceptance rate forces the sampler to take smaller steps, keeping the variances within reasonable ranges. The concern with setting the target acceptance this high is that for parameters with wider posteriors, the sampler will not be able to move fast enough. In the NUTS algorithm, at each iteration, the sampler looks for a place to “U-Turn” in a series of possible branches. If the sampler is terminating before the maximum possible tree depth (set to 15; see Section 5.1), then the algorithm is able to adequately find good values for the next iteration of the chain. Bumping up against the maximum allowed tree depth, or going beyond it, indicates that stepsize is too small (Stan Development Team, 2016a, 2016b). Because the Max Treedepth values in Table 5.1 are all below the maximum specified, and the BFMI values are close to 1, there is strong evidence that the sampler was indeed able to adequately sample the posteriors. 5.3 Model fit To assess model fit, I will use posterior predictive model checks. Posterior predictive checks involve simulating replications of the data using the values of the Markov chain, and then comparing the replicated data to the observed data (A. Gelman et al., 2014). This means that replicated data sets take into account the uncertainty in the parameter estimates, as a new replicated data set is created at each iteration of the Markov chain. These data sets can then be used to look for systematic differences in the characterics of the observed and simulated data, often through visualizations (A. Gelman &amp; Hill, 2006). The first step is to extract the replicated data sets from the stanfit object using the purrr (Wickham, 2016b) and tidyr (Wickham, 2017) packages. library(purrr) library(tidyr) home_rep &lt;- rstan::extract(gri_stanfit, pars = &quot;home_rep&quot;, permuted = TRUE)$home_rep away_rep &lt;- rstan::extract(gri_stanfit, pars = &quot;away_rep&quot;, permuted = TRUE)$away_rep home_rep &lt;- t(home_rep) %&gt;% as_data_frame() %&gt;% as.list() away_rep &lt;- t(away_rep) %&gt;% as_data_frame() %&gt;% as.list() counter &lt;- seq_along(home_rep) rep_data &lt;- pmap_df(.l = list(h = home_rep, a = away_rep, c = counter), .f = function(h, a, c) { data_frame( replication = c, game = seq_len(length(h)), home_score = h, away_score = a ) }) 5.3.1 Score distributions The first posterior predictive check to be examined is the distribution of home and away scores. For each replicated data set there is a distribution of goals scored by the home and away teams. To compare to the observed data, we can plot each of these distributions, and then overlay the distribution from the observed data using ggplot2 (Wickham &amp; Chang, 2016). The code for Figure 5.3 can be see in Appendix C.4. Figure 5.3: Recovery of observed score distributions. Figure 5.3 shows that the observed score distributions for both the home and away scores are very similar to what is seen in the replicated data sets. Thus, this provides evidence that the model is able to recover the observed distributions. 5.3.2 Margin of victory intervals It is also possible to examine the margin of victory for each game. In each of the replicated data sets, the home team’s margin of victory for every game can be calculated as replicated home score minus the replicated away score. Doing this for every replication creates a posterior distribution for the home team’s margin of victory in every game. From the posterior we can create credible intervals (Appendix C.5) and determine how often the observed margin of victory falls outside the credible interval. Figure 5.4: Example credible interval for game margin of victories. Figure 5.4 shows examples of games where the observed margin of victory fell inside the 50% credible interval, inside the 95% credible interval, and outside both intervals. Overall, for the 6608 games included in the estimation, the observed margin of victory fell within the 50% credible interval 70.6 percent of the time and within the 95% credible 98.0 percent of the time. 5.3.3 Prediction error Another posterior check that can be looked at is prediction accuracy. For each replication, whether the home team won, lost, or tied can be determined for each game. Across replications, it is then possible to look at the probability of the home teaming experiencing a given outcome in each game. The probabilities can then be compared to the observed outcome to determine how accurate the predictions were. The accuracy can be determined by using a binary loss function (was the most likely outcome the observed outcome), or a log loss function (how far from the observed outcome was the probability of that outcome). The first step is to get the probability of the home team winning, tieing, and losing each game. outcomes &lt;- rep_data %&gt;% mutate(mov = home_score - away_score) %&gt;% group_by(game) %&gt;% summarize( win_prob = length(which(mov &gt; 0)) / n(), tie_prob = length(which(mov == 0)) / n(), loss_prob = length(which(mov &lt; 0)) / n() ) %&gt;% mutate( most_likely = ifelse(win_prob &gt; tie_prob &amp; win_prob &gt; loss_prob, &quot;win&quot;, ifelse(tie_prob &gt; win_prob &amp; tie_prob &gt; loss_prob, &quot;tie&quot;, &quot;loss&quot;)) ) %&gt;% mutate( obs_mov = fit_data$h_goals - fit_data$a_goals, outcome = ifelse(obs_mov &gt; 0, &quot;win&quot;, ifelse(obs_mov == 0, &quot;tie&quot;, &quot;loss&quot;)) ) Using the predictions from the replicated data sets, the model gave the observed outcome the highest probability in 53.0 percent of the games. One problem with this approach is that it doesn’t take into the actual values of the probabilities. For example, take two games where the home team won. In the first game, the home team had a 60 percent chance of winning. In the second game, the home teams had a 90 percent chance of winning. In both cases, the most likely outcome matches the observed outcome, so both instances are assigned a one in the binary loss function as a correct prediction. Alternatively, we could use the log loss function to look at how far the probability was from the observed event (Altun, Johnson, &amp; Hofmann, 2003). In this example, the second game would be a better prediction, and have a lower log loss, because a probability of 0.9 is closer to the observed outcome (1) than a probability of 0.6. The log loss for multiple games is defined in equation (5.1). \\[\\begin{equation} logloss = - \\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}) \\tag{5.1} \\end{equation}\\] In equation (5.1), \\(N\\) is number of observations (in this case the number of games), \\(M\\) is the number of outcomes (for soccer games this is 3: win, loss, and tie), \\(y_{ij}\\) is a binary indicator of whether outcome \\(M\\) occured for observation \\(N\\) (1) or not (0), and \\(p_{ij}\\) is the probability of outcome \\(M\\) for observation \\(N\\). Thus, the log loss for a set of predictions is the log of the probability of the observed outcomes, summed over all observations. In the case of perfect predictions, the probability of the observed outcome would be 1, and the log probability would be 0. This means that the closer the log loss is to 0, the better the predictions are (Roy &amp; McCallum, 2001). To calculate the log loss for the estimated game random intercept model, I define a function that takes in a matrix of predictions and a matrix of outcomes. logloss &lt;- function(pred, obs){ eps &lt;- 1e-15 pred &lt;- pmin(pmax(pred, eps), 1 - eps) (-1 / nrow(obs)) * sum(obs * log(pred) + (1 - obs) * log(1 - pred)) } Then, matrices of predictions and outcomes can be created, and the log loss can be calculated. predictions &lt;- outcomes %&gt;% select(win_prob, tie_prob, loss_prob) %&gt;% as.matrix() observations &lt;- outcomes %&gt;% mutate( obs_win = ifelse(outcome == &quot;win&quot;, 1, 0), obs_tie = ifelse(outcome == &quot;tie&quot;, 1, 0), obs_loss = ifelse(outcome == &quot;loss&quot;, 1, 0) ) %&gt;% select(obs_win, obs_tie, obs_loss) %&gt;% as.matrix() avg_logloss &lt;- logloss(pred = predictions, obs = observations) The log loss for predictions from the replicated data sets is 1.722. Converting back to a probability scale, on average, the probability of the observed outcome was off by 0.18. In isolation, the log loss can be hard to interpret. Instead, it is often useful to compare to baseline models. Table 5.2: Log loss comparison to baseline models. Model Log Loss Game Random Intercept 1.72 Data Average 1.86 Equal Probabilities 1.91 Home Win 37.67 Table 5.2 shows the log loss for a variety of models. In the data average model, the probability of each outcome is set to the overall average for the entire data set. In the equal probabilities model, the probability for each outcome is set to 0.33. Finally, in the home win model, the probability of the home team winning is set to 1 and the probability of the other outcomes is set to 0. The posterior predictive probabilities from the game random intercept model out perform all of these baseline models. 5.3.4 Posterior predictive check summary Overall, the posterior predictive model checks indicate adequate model fit. The model is able to accurately recover the distributions of scores for both the home and away teams (Figure 5.3). Additionally, when looking at individual games, the credible intervals for the margin of victory are able to consistently capture the observed margin of victory. Finally, the prediction error shows that the model’s predictions are able to more accurately pick game outcomes than a variety of baseline models. Taken in totality, there is sufficient evidence of model fit for us to proceed with the analysis and examine the posterior distributions of the parameter estimates. 5.4 Results A ranking of the teams can be created by the goals they would be expected to score and concede against an average team at a neutral location. The larger the difference between these expected goals, the better team. First we pull out the parameters we need from the model. params &lt;- rstan::extract(gri_stanfit, pars = c(&quot;mu&quot;, &quot;alpha&quot;, &quot;delta&quot;)) alpha &lt;- colMeans(params$alpha) delta &lt;- colMeans(params$delta) mu &lt;- mean(params$mu) Then we can compute the expected offense, defense, and margin for each team to create the rankings. rankings &lt;- data_frame( Club = team_counts$team, Attacking = alpha, Defense = delta ) %&gt;% mutate( `Expected Offense` = exp(mu + alpha), `Expected Defense` = exp(mu + delta), `Expected Margin` = `Expected Offense` - `Expected Defense` ) %&gt;% arrange(desc(`Expected Margin`)) %&gt;% mutate( Attacking = formatC(Attacking, digits = 3, drop0trailing = FALSE, format = &quot;f&quot;), Defense = formatC(Defense, digits = 3, drop0trailing = FALSE, format = &quot;f&quot;), `Expected Offense` = formatC(`Expected Offense`, digits = 2, drop0trailing = FALSE, format = &quot;f&quot;), `Expected Defense` = formatC(`Expected Defense`, digits = 2, drop0trailing = FALSE, format = &quot;f&quot;), `Expected Margin` = formatC(`Expected Margin`, digits = 2, drop0trailing = FALSE, format = &quot;f&quot;) ) As we might expect, the top of the list is dominated by team leading the best European leagues and having success in the Champions League. Barcelona comes in as the top team in rankings, followed by Bayern Munich, AS Monaco, Paris Saint-Germain, and Real Madrid to round out the top 5. The top offense belongs to Barcelona according to the model, while Bayern Munich boasts the best defense. DT::datatable(select(rankings, -Club), rownames = rankings$Club, options = list(pageLength = 10, scrollX = TRUE, columnDefs = list(list(className = &#39;dt-center&#39;, targets = 1:5))), caption = &quot;Club rankings from game random intercept model&quot;) References "],
["predict.html", "6 Making Predictions 6.1 Predict individual games 6.2 Predict domestic leagues 6.3 UEFA Champions League", " 6 Making Predictions To make predictions, I will use the retained iterations from the gri_stanfit object. Alternatively, I could use the means of the posterior distributions to calculate a single lambda for each score within each game. These lambda values could then be used to generate a sample of scores which could be used to predict the outcome of the game. However, this approach would ignore the uncertainty in the parameter estimates. A better solution would be to calculate lambda values at each iteration of the chain, using the current estimates of the parameters. Thus, at each iteration, it is possible to simulate a score for each team at each iteration, creating a posterior distribution for the score of each game. 6.1 Predict individual games First, I will extract the parameters we need from the fitted model, and load in the team codes associated with each team. model_params &lt;- rstan::extract(gri_stanfit, pars = c(&quot;mu&quot;, &quot;eta&quot;, &quot;alpha&quot;, &quot;delta&quot;, &quot;sigma_g&quot;)) load(&quot;_data/team_counts.rda&quot;) I then use the predict_games function (see Appendix D.1) to predict the outcome of a game between any two teams included in the model. For example, we can predict the winner of a game between Barcelona and Real Madrid played in Barcelona. library(dplyr) library(ggplot2) library(purrr) prediction &lt;- predict_game(home = &quot;Barcelona&quot;, away = &quot;Real Madrid&quot;, neutral = FALSE, visualize = TRUE, team_codes = team_counts, chains = model_params) prediction$predictions %&gt;% select(Club = club, `Expected Goals` = expected_goals, `Win Probability` = prob_win, `Tie Probability` = prob_tie) %&gt;% knitr::kable(caption = &quot;Prediction for Real Madrid at Barcelona&quot;, align = &quot;c&quot;, digits = 3) Table 6.1: Prediction for Real Madrid at Barcelona Club Expected Goals Win Probability Tie Probability Barcelona 2.62 0.586 0.176 Real Madrid 1.59 0.238 0.176 Because I specified visualize = TRUE in the call to predict_game we can use the multiplot function (Appendix D.2) to visualize the range of possible outcomes from the posteriors. library(grid) multiplot(plotlist = prediction$plots, cols = 2) Figure 6.1: Visualizations for Real Madrid at Barcelona 6.2 Predict domestic leagues To predict entire leagues, I follow the same general process, simulating an outcome for each retained iteration of the chain. The difference for leagues is that instead of simulating a single game at each iteration, we simulate the remainder of the league season, and calculate the league winner. This is all done by the predict_league function (Appendix D.3). In order to simulate these outcome, I’ll first need to load in the full data set that includes future games library(lubridate) library(rvest) library(tidyr) library(scales) load(&quot;_data/full_data.rda&quot;) load(&quot;_data/club_rankings.rda&quot;) Then, I can use the predict_league function to get championship probabilities for each league. 6.2.1 English Premier League predict_league(league = &quot;Premier League&quot;, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = &quot;club&quot;) %&gt;% arrange(desc(champ_pct)) %&gt;% mutate(champ_pct = percent(ifelse(is.na(champ_pct), 0, champ_pct))) %&gt;% select(Club = club, Offense = exp_offense, Defense = exp_defense, `Expected Points` = sim_points, `Championship Probability` = champ_pct) %&gt;% knitr::kable(caption = &quot;Premier League Championship Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.2: Premier League Championship Probabilities Club Offense Defense Expected Points Championship Probability Chelsea 1.83 0.77 86.5 90.0% Tottenham Hotspur 1.71 0.75 78.4 7.3% Manchester City 1.77 0.82 75.0 1.9% Liverpool 1.74 0.82 74.0 0.7% Arsenal 1.95 0.91 68.9 0.0% Manchester United 1.56 0.74 69.0 0.0% Everton 1.38 0.87 62.9 0.0% West Bromwich Albion 1.18 1.01 53.5 0.0% Stoke City 1.07 1.04 46.7 0.0% Southampton 1.09 0.88 45.0 0.0% AFC Bournemouth 1.22 1.21 42.9 0.0% West Ham United 1.18 1.24 42.5 0.0% Burnley 1.02 0.96 43.1 0.0% Watford 1.04 1.11 40.7 0.0% Leicester City 1.20 1.11 42.2 0.0% Crystal Palace 1.11 1.12 37.6 0.0% Swansea City 1.29 1.25 38.2 0.0% Hull City 1.07 1.12 34.9 0.0% Middlesbrough 1.01 0.88 34.2 0.0% Sunderland 0.87 1.13 29.4 0.0% 6.2.2 French Ligue 1 predict_league(league = &quot;Ligue 1&quot;, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = &quot;club&quot;) %&gt;% arrange(desc(champ_pct)) %&gt;% mutate(champ_pct = percent(ifelse(is.na(champ_pct), 0, champ_pct))) %&gt;% select(Club = club, Offense = exp_offense, Defense = exp_defense, `Expected Points` = sim_points, `Championship Probability` = champ_pct) %&gt;% knitr::kable(caption = &quot;Ligue 1 Championship Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.3: Ligue 1 Championship Probabilities Club Offense Defense Expected Points Championship Probability AS Monaco 2.12 0.89 88.3 72.2% Paris Saint-Germain 1.90 0.75 86.1 27.7% Nice 1.20 0.93 75.4 0.1% Lyon 1.65 0.96 67.0 0.0% Marseille 1.25 1.00 58.8 0.0% Bordeaux 1.16 1.08 57.8 0.0% St Etienne 0.92 0.83 54.3 0.0% Stade Rennes 0.86 1.06 48.4 0.0% Angers 1.00 1.05 48.1 0.0% Guingamp 1.11 1.05 49.7 0.0% Nantes 0.88 1.12 47.4 0.0% Toulouse 1.00 0.93 48.1 0.0% Metz 0.87 1.25 44.2 0.0% Lille 0.88 1.01 44.2 0.0% Montpellier 1.19 1.36 42.8 0.0% Caen 0.97 1.32 41.1 0.0% Dijon FCO 1.08 1.19 37.7 0.0% AS Nancy Lorraine 0.79 0.99 36.8 0.0% Bastia 0.80 1.21 32.6 0.0% Lorient 1.02 1.42 34.6 0.0% 6.2.3 German Bundesliga predict_league(league = &quot;Bundesliga&quot;, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = &quot;club&quot;) %&gt;% arrange(desc(champ_pct)) %&gt;% mutate(champ_pct = percent(ifelse(is.na(champ_pct), 0, champ_pct))) %&gt;% select(Club = club, Offense = exp_offense, Defense = exp_defense, `Expected Points` = sim_points, `Championship Probability` = champ_pct) %&gt;% knitr::kable(caption = &quot;Bundesliga Championship Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.4: Bundesliga Championship Probabilities Club Offense Defense Expected Points Championship Probability Bayern Munich 2.09 0.68 82.1 100% RB Leipzig 1.39 0.93 63.4 0% Borussia Dortmund 1.81 0.90 62.4 0% TSG Hoffenheim 1.41 0.91 58.7 0% Hertha Berlin 1.18 0.94 53.9 0% FC Cologne 1.22 0.96 50.3 0% Eintracht Frankfurt 0.96 0.88 47.6 0% SC Freiburg 1.13 1.18 46.3 0% Schalke 04 1.32 0.87 48.1 0% Borussia Monchengladbach 1.25 0.97 45.0 0% Bayer Leverkusen 1.28 1.03 43.5 0% Mainz 1.16 1.16 39.5 0% Werder Bremen 1.16 1.23 39.2 0% FC Augsburg 0.93 1.05 38.2 0% VfL Wolfsburg 0.90 0.98 39.0 0% Hamburg SV 0.97 1.22 37.0 0% FC Ingolstadt 04 0.90 1.10 29.9 0% SV Darmstadt 98 0.79 1.27 22.4 0% 6.2.4 Italian Serie A predict_league(league = &quot;Serie A&quot;, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = &quot;club&quot;) %&gt;% arrange(desc(champ_pct)) %&gt;% mutate(champ_pct = percent(ifelse(is.na(champ_pct), 0, champ_pct))) %&gt;% select(Club = club, Offense = exp_offense, Defense = exp_defense, `Expected Points` = sim_points, `Championship Probability` = champ_pct) %&gt;% knitr::kable(caption = &quot;Serie A Championship Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.5: Serie A Championship Probabilities Club Offense Defense Expected Points Championship Probability Juventus 1.61 0.71 90.8 90.1% AS Roma 1.71 0.87 82.4 8.7% Napoli 1.70 0.98 78.2 1.2% Lazio 1.39 0.88 72.2 0.0% Internazionale 1.43 1.08 69.3 0.0% Atalanta 1.28 0.95 68.8 0.0% AC Milan 1.22 0.95 67.6 0.0% Fiorentina 1.34 1.02 62.9 0.0% Sampdoria 1.03 0.99 52.1 0.0% Torino 1.49 1.14 54.5 0.0% Chievo Verona 1.04 1.04 49.6 0.0% Udinese 1.07 1.06 47.6 0.0% Bologna 0.98 1.08 44.6 0.0% Cagliari 1.13 1.35 43.5 0.0% Sassuolo 1.11 1.18 41.7 0.0% Genoa 1.01 1.18 38.5 0.0% Empoli 0.72 1.14 30.8 0.0% Palermo 0.82 1.31 24.5 0.0% Crotone 0.77 1.21 21.6 0.0% US Pescara 0.93 1.46 20.9 0.0% 6.2.5 Spanish La Liga predict_league(league = &quot;La Liga&quot;, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = &quot;club&quot;) %&gt;% arrange(desc(champ_pct)) %&gt;% mutate(champ_pct = percent(ifelse(is.na(champ_pct), 0, champ_pct))) %&gt;% select(Club = club, Offense = exp_offense, Defense = exp_defense, `Expected Points` = sim_points, `Championship Probability` = champ_pct) %&gt;% knitr::kable(caption = &quot;La Liga Championship Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.6: La Liga Championship Probabilities Club Offense Defense Expected Points Championship Probability Barcelona 2.28 0.84 85.3 51.6% Real Madrid 2.03 0.95 85.6 47.8% Sevilla FC 1.36 0.94 74.0 0.3% Atletico Madrid 1.45 0.73 73.6 0.3% Villarreal 1.08 0.81 63.0 0.0% Real Sociedad 1.27 1.06 62.7 0.0% Athletic Bilbao 1.10 0.96 57.8 0.0% Eibar 1.26 1.08 54.1 0.0% Espanyol 1.15 0.99 54.6 0.0% Alavés 0.99 0.87 53.9 0.0% Celta Vigo 1.28 0.99 54.0 0.0% Las Palmas 1.28 1.17 47.8 0.0% Real Betis 0.94 1.14 41.6 0.0% Valencia 1.19 1.28 43.0 0.0% Málaga 1.04 1.23 37.6 0.0% Deportivo La Coruña 1.03 1.11 39.6 0.0% Leganes 0.79 1.10 36.2 0.0% Sporting Gijón 0.97 1.35 30.9 0.0% Granada 0.85 1.37 27.6 0.0% Osasuna 0.87 1.47 20.1 0.0% 6.3 UEFA Champions League Simulating the UEFA Champions League is very similar to the process used for simulating the domestic leagues. At each retained iteration of the MCMC chain, I simulate the remainder of the Champions League matches. Because there isn’t a true bracket, and the opponents are drawn randomly before each round, I first define the current match-ups. matchups &lt;- list( c(&quot;Barcelona&quot;, &quot;Juventus&quot;), c(&quot;AS Monaco&quot;, &quot;Borussia Dortmund&quot;), c(&quot;Bayern Munich&quot;, &quot;Real Madrid&quot;), c(&quot;Leicester City&quot;, &quot;Atletico Madrid&quot;) ) I can then use the predict_ucl function (Appendix D.4) to calculate the probability of each team advancing to each subsequent round. predict_ucl(matchups = matchups, games = full_data, chains = model_params, team_codes = team_counts) %&gt;% left_join(select(club_rankings, club, exp_offense, exp_defense), by = c(&quot;Club&quot; = &quot;club&quot;)) %&gt;% arrange(desc(Champion)) %&gt;% mutate(Quarterfinals = percent(Quarterfinals), Semifinals = percent(Semifinals), Final = percent(Final), Champion = percent(Champion)) %&gt;% select(Club, Offense = exp_offense, Defense = exp_defense, Quarterfinals, Semifinals, Final, Champion) %&gt;% knitr::kable(caption = &quot;UEFA Champions League Probabilities&quot;, align = &quot;c&quot;, digits = 2) Table 6.7: UEFA Champions League Probabilities Club Offense Defense Quarterfinals Semifinals Final Champion Bayern Munich 2.09 0.68 100% 65.9% 44.2% 26.7% Barcelona 2.28 0.84 100% 57.5% 32.5% 17.4% AS Monaco 2.12 0.89 100% 60.0% 31.3% 15.2% Atletico Madrid 1.45 0.73 100% 74.5% 29.8% 13.3% Juventus 1.61 0.71 100% 42.5% 22.5% 10.0% Real Madrid 2.03 0.95 100% 34.1% 17.0% 8.6% Borussia Dortmund 1.81 0.90 100% 40.0% 17.8% 7.5% Leicester City 1.20 1.11 100% 25.5% 4.9% 1.4% "],
["conclusion.html", "7 Conclusion 7.1 Limitations of the current approach 7.2 Future Directions", " 7 Conclusion 7.1 Limitations of the current approach 7.2 Future Directions "],
["simulation-functions.html", "A Simulation Functions A.1 Generate bivariate Poisson A.2 Generate game random intercept A.3 Fit models to simualted data A.4 Plot correlation matrices", " A Simulation Functions A.1 Generate bivariate Poisson generate_bivpois &lt;- function(run, seeds, num_club) { setSeeds(seeds, run = run) teams &lt;- data_frame( club = paste0(&quot;club&quot;, sprintf(&quot;%02d&quot;, seq_len(num_club))), attack = rnorm(n = num_club, mean = 0, sd = 0.35), defend = rnorm(n = num_club, mean = 0, sd = 0.35), cov = rnorm(n = num_club, mean = 0, sd = 0.1) ) games &lt;- teams %&gt;% select(club) %&gt;% flatten_chr() %&gt;% crossing(., .) colnames(games) &lt;- c(&quot;home&quot;, &quot;away&quot;) games &lt;- games %&gt;% filter(home != away) %&gt;% left_join(teams, by = c(&quot;home&quot; = &quot;club&quot;)) %&gt;% rename(h_att = attack, h_def = defend, h_cov = cov) %&gt;% left_join(teams, by = c(&quot;away&quot; = &quot;club&quot;)) %&gt;% rename(a_att = attack, a_def = defend, a_cov = cov) %&gt;% mutate( lambda1 = exp(0 + 0.5 + h_att + a_def), lambda2 = exp(0 + a_att + h_def), lambda3 = exp(h_cov + a_cov), h_goals = rpois(n = nrow(.), lambda = (lambda1 + lambda3)), a_goals = rpois(n = nrow(.), lambda = (lambda2 + lambda3)), home_game = 1 ) %&gt;% select(home, away, h_goals, a_goals, home_game) list( method = &quot;bivpois&quot;, teams = teams, games = games ) } A.2 Generate game random intercept generate_gri &lt;- function(run, seeds, num_club) { setSeeds(seeds, run = run) teams &lt;- data_frame( club = paste0(&quot;club&quot;, sprintf(&quot;%02d&quot;, seq_len(num_club))), attack = rnorm(n = num_club, mean = 0, sd = 0.35), defend = rnorm(n = num_club, mean = 0, sd = 0.35) ) games &lt;- teams %&gt;% select(club) %&gt;% flatten_chr() %&gt;% crossing(., .) colnames(games) &lt;- c(&quot;home&quot;, &quot;away&quot;) games &lt;- games %&gt;% filter(home != away) %&gt;% left_join(teams, by = c(&quot;home&quot; = &quot;club&quot;)) %&gt;% rename(h_att = attack, h_def = defend) %&gt;% left_join(teams, by = c(&quot;away&quot; = &quot;club&quot;)) %&gt;% rename(a_att = attack, a_def = defend) %&gt;% mutate( gamma = rnorm(n = nrow(.), mean = 0, sd = 0.1), lambda1 = exp(0 + 0.5 + h_att + a_def + gamma), lambda2 = exp(0 + a_att + h_def + gamma), h_goals = rpois(n = nrow(.), lambda = (lambda1)), a_goals = rpois(n = nrow(.), lambda = (lambda2)), home_game = 1 ) %&gt;% select(home, away, h_goals, a_goals, home_game) list( method = &quot;gri&quot;, teams = teams, games = games ) } A.3 Fit models to simualted data simulation_fun &lt;- function(x) { team_codes &lt;- data_frame( club = sort(unique(c(x$games$home, x$games$away))) ) %&gt;% mutate(code = seq_len(nrow(.))) fit_data &lt;- left_join(x$games, team_codes, by = c(&quot;home&quot; = &quot;club&quot;)) %&gt;% rename(home_code = code) %&gt;% left_join(team_codes, by = c(&quot;away&quot; = &quot;club&quot;)) %&gt;% rename(away_code = code) stan_data &lt;- list( num_clubs = nrow(team_codes), num_games = nrow(fit_data), home = fit_data$home_code, away = fit_data$away_code, h_goals = fit_data$h_goals, a_goals = fit_data$a_goals, homeg = fit_data$home_game ) bivpois &lt;- stan(file = &quot;_data/stan-models/biv_pois.stan&quot;, data = stan_data, chains = 2, iter = 15000, warmup = 5000, init = &quot;random&quot;, thin = 1, cores = 2, control = list(adapt_delta = 0.99)) gri &lt;- stan(file = &quot;_data/stan-models/gri.stan&quot;, data = stan_data, chains = 2, iter = 15000, warmup = 5000, init = &quot;random&quot;, thin = 1, cores = 2, control = list(adapt_delta = 0.99)) bivpois_maxrhat &lt;- as.data.frame(summary(bivpois)[[1]]) %&gt;% select(Rhat) %&gt;% flatten_dbl() %&gt;% max() gri_maxrhat &lt;- as.data.frame(summary(gri)[[1]]) %&gt;% select(Rhat) %&gt;% flatten_dbl() %&gt;% max() bivpois_params &lt;- rstan::extract(bivpois, pars = c(&quot;mu&quot;, &quot;eta&quot;, &quot;alpha&quot;, &quot;delta&quot;, &quot;rho&quot;)) gri_params &lt;- rstan::extract(gri, pars = c(&quot;mu&quot;, &quot;eta&quot;, &quot;alpha&quot;, &quot;delta&quot;)) bivpois_alpha &lt;- colMeans(bivpois_params$alpha) bivpois_delta &lt;- colMeans(bivpois_params$delta) gri_alpha &lt;- colMeans(gri_params$alpha) gri_delta &lt;- colMeans(gri_params$delta) bivpois_alpha_bias &lt;- mean(bivpois_alpha - x$teams$attack) bivpois_delta_bias &lt;- mean(bivpois_delta - x$teams$defend) bivpois_alpha_mse &lt;- mean((bivpois_alpha - x$teams$attack)^2) bivpois_delta_mse &lt;- mean((bivpois_delta - x$teams$defend)^2) gri_alpha_bias &lt;- mean(gri_alpha - x$teams$attack) gri_delta_bias &lt;- mean(gri_delta - x$teams$defend) gri_alpha_mse &lt;- mean((gri_alpha - x$teams$attack)^2) gri_delta_mse &lt;- mean((gri_delta - x$teams$defend)^2) data_frame( generator = x$method, true_params = list(x$teams), bivpois_rhat = bivpois_maxrhat, bivpois_params = list(list(bivpois_alpha = bivpois_alpha, bivpois_delta = bivpois_delta)), bivpois_alpha_bias = bivpois_alpha_bias, bivpois_delta_bias = bivpois_delta_bias, bivpois_alpha_mse = bivpois_alpha_mse, bivpois_delta_mse = bivpois_delta_mse, gri_rhat = gri_maxrhat, gri_params = list(list(gri_alpha = gri_alpha, gri_delta = gri_delta)), gri_alpha_bias, gri_delta_bias, gri_alpha_mse, gri_delta_mse ) } A.4 Plot correlation matrices A.4.1 Plot components lowerFn &lt;- function(data, mapping, ..., alpha = 0.5, lim = 2) { p &lt;- ggplot(data = data, mapping = mapping) + geom_point(..., alpha = alpha) + geom_abline(intercept = 0, slope = 1, color = &quot;black&quot;) + scale_x_continuous(limits = c(-lim, lim), breaks = seq(-lim, lim, 1)) + scale_y_continuous(limits = c(-lim, lim), breaks = seq(-lim, lim, 1)) + theme_bw() p } diagFn &lt;- function(data, mapping, ..., alpha = 0.5, lim = 2) { p &lt;- ggplot(data = data, mapping = mapping) + geom_density(..., alpha = alpha) + scale_x_continuous(limits = c(-lim, lim), breaks = seq(-lim, lim, 1)) + theme_bw() + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) p } upperFn &lt;- function(data, mapping, ..., size = 3, lim = 2) { xCol &lt;- deparse(mapping$x) yCol &lt;- deparse(mapping$y) colorCol &lt;- deparse(mapping$colour) xVal &lt;- data[[xCol]] yVal &lt;- data[[yCol]] cVal &lt;- data[[colorCol]] plot_data &lt;- data_frame(xVal, yVal, cVal) %&gt;% group_by(cVal) %&gt;% summarize(corr = cor(xVal, yVal, method = &quot;pearson&quot;)) %&gt;% mutate( cVal = factor(cVal, levels = c(&quot;gri&quot;, &quot;bivpois&quot;), labels =c(&quot;GRI&quot;, &quot;Biv Poisson&quot;)), lab = paste0(cVal, &quot;: &quot;, sprintf(&quot;%.3f&quot;, corr)), y_pos = 1:2, x_pos = 0 ) p &lt;- ggplot(data = plot_data, aes(x = x_pos, y = y_pos, fill = cVal, label = lab)) + geom_label(color = &quot;white&quot;, fontface = &quot;bold&quot;, size = size) + scale_x_continuous(limits = c(-lim, lim), breaks = seq(-lim, lim, 1)) + scale_y_continuous(limits = c(0, 3)) + scale_fill_manual(values = c(`Biv Poisson` = &quot;#F8766D&quot;, GRI = &quot;#00BFC4&quot;)) + theme_bw() + theme( legend.position = &quot;none&quot; ) p } get_lim &lt;- function(data) { data %&gt;% as.list() %&gt;% flatten_dbl() %&gt;% abs() %&gt;% max() %&gt;% round_any(accuracy = 0.5, f = ceiling) } A.4.2 Plot creation lim &lt;- get_lim(select(plot_sim, -generator)) ggpairs( title = &quot;Alpha Recovery&quot;, data = plot_sim, columns = c(&quot;true_alpha&quot;, &quot;bivpois_alpha&quot;, &quot;gri_alpha&quot;), mapping = aes(color = generator, fill = generator), columnLabels = c(&quot;True&quot;, &quot;Bivariate Poisson&quot;, &quot;Game Random Intercept&quot;), upper = list(continuous = wrap(upperFn, size = 3, lim = lim)), lower = list(continuous = wrap(lowerFn, alpha = 0.1, lim = lim)), diag = list(continuous = wrap(diagFn, alpha = 0.5, lim = lim)) ) ggpairs( title = &quot;Delta Recovery&quot;, data = plot_sim, columns = c(&quot;true_delta&quot;, &quot;bivpois_delta&quot;, &quot;gri_delta&quot;), mapping = aes(color = generator, fill = generator), columnLabels = c(&quot;True&quot;, &quot;Bivariate Poisson&quot;, &quot;Game Random Intercept&quot;), upper = list(continuous = wrap(upperFn, size = 3, lim = lim)), lower = list(continuous = wrap(lowerFn, alpha = 0.1, lim = lim)), diag = list(continuous = wrap(diagFn, alpha = 0.5, lim = lim)) ) "],
["scrape-functions.html", "B Web Scraping Functions B.1 Scrape league pages B.2 Scrape international cups B.3 Scrape domestic cups B.4 Scrape games", " B Web Scraping Functions B.1 Scrape league pages scrape_league &lt;- function(x) { cont &lt;- TRUE while(cont) { url_data &lt;- safe_read_html(x) if(is.null(url_data[[1]])) { closeAllConnections() Sys.sleep(5) } else { url_data &lt;- url_data[[1]] cont &lt;- FALSE } } league_table &lt;- url_data %&gt;% html_nodes(css = &quot;table&quot;) %&gt;% html_table() league_table &lt;- league_table[[1]] colnames(league_table) &lt;- as.character(league_table[1,]) colnames(league_table) &lt;- make.names(colnames(league_table), unique = TRUE) league_table &lt;- league_table[-1,] league_table &lt;- league_table %&gt;% select(club = TEAM, goals_for = `F`, goals_against = A, points = PTS) %&gt;% mutate(club = trimws(club, which = &quot;both&quot;)) teams &lt;- url_data %&gt;% html_nodes(&quot;td a&quot;) %&gt;% html_text() %&gt;% as.character() %&gt;% trimws(which = &quot;both&quot;) team_urls &lt;- url_data %&gt;% html_nodes(&quot;td a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% as.character() league_table &lt;- league_table %&gt;% left_join(data_frame(club = teams, club_url = team_urls), by = &quot;club&quot;) %&gt;% as_data_frame() return(league_table) } B.2 Scrape international cups scrape_major_cup &lt;- function(x) { cont &lt;- TRUE while(cont) { url_data &lt;- safe_read_html(x) if(is.null(url_data[[1]])) { closeAllConnections() Sys.sleep(5) } else { url_data &lt;- url_data[[1]] cont &lt;- FALSE } } league_table &lt;- url_data %&gt;% html_nodes(css = &quot;table&quot;) %&gt;% html_table() league_table &lt;- map_df(.x = league_table, .f = function(x) { colnames(x) &lt;- as.character(x[1,]) colnames(x) &lt;- make.names(colnames(x), unique = TRUE) x &lt;- x[-1,] x &lt;- x %&gt;% select(club = TEAM) return(x) }) %&gt;% bind_rows() %&gt;% mutate(club = trimws(club, which = &quot;both&quot;)) teams &lt;- url_data %&gt;% html_nodes(&quot;td a&quot;) %&gt;% html_text() %&gt;% as.character() %&gt;% trimws(which = &quot;both&quot;) team_urls &lt;- url_data %&gt;% html_nodes(&quot;td a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% as.character() league_table &lt;- league_table %&gt;% left_join(data_frame(club = teams, club_url = team_urls), by = &quot;club&quot;) %&gt;% as_data_frame() return(league_table) } B.3 Scrape domestic cups scrape_dom_cup &lt;- function(x) { cont &lt;- TRUE while(cont) { url_data &lt;- safe_read_html(x) if(is.null(url_data[[1]])) { closeAllConnections() Sys.sleep(5) } else { url_data &lt;- url_data[[1]] cont &lt;- FALSE } } teams &lt;- url_data %&gt;% html_nodes(&quot;#stats-fair-play a&quot;) %&gt;% html_text() %&gt;% as.character() %&gt;% trimws(which = &quot;both&quot;) team_urls &lt;- url_data %&gt;% html_nodes(&quot;#stats-fair-play a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% as.character() data_frame( club = teams, club_url = team_urls ) } B.4 Scrape games scrape_team &lt;- function(x, y) { x &lt;- gsub(&quot;/index&quot;, &quot;/fixtures&quot;, x, fixed = TRUE) cont &lt;- TRUE while(cont) { url_data &lt;- safe_read_html(x) if(is.null(url_data[[1]])) { closeAllConnections() Sys.sleep(5) } else { url_data &lt;- url_data[[1]] cont &lt;- FALSE } } date &lt;- url_data %&gt;% html_nodes(&quot;.headline&quot;) %&gt;% html_text() %&gt;% as.character() if (&quot;LIVE&quot; %in% date) { date[which(date == &quot;LIVE&quot;)] &lt;- format(Sys.Date(), &quot;%b %d, %Y&quot;) } date &lt;- mdy(date) home_team &lt;- url_data %&gt;% html_nodes(&quot;.score-home-team .team-name&quot;) %&gt;% html_text() %&gt;% as.character() away_team &lt;- url_data %&gt;% html_nodes(&quot;.score-away-team .team-name&quot;) %&gt;% html_text() %&gt;% as.character() home_score &lt;- url_data %&gt;% html_nodes(&quot;.home-score&quot;) %&gt;% html_text() %&gt;% as.character() %&gt;% gsub(&quot; &quot;, &quot;&quot;, x = .) %&gt;% gsub( &quot; *\\\\(.*?\\\\) *&quot;, &quot;&quot;, x = .) %&gt;% as.numeric() away_score &lt;- url_data %&gt;% html_nodes(&quot;.away-score&quot;) %&gt;% html_text() %&gt;% as.character() %&gt;% gsub(&quot; &quot;, &quot;&quot;, x = .) %&gt;% gsub( &quot; *\\\\(.*?\\\\) *&quot;, &quot;&quot;, x = .) %&gt;% as.numeric() competition &lt;- url_data %&gt;% html_nodes(&quot;.score-column.score-competition&quot;) %&gt;% html_text() %&gt;% as.character() team_data &lt;- data_frame( date = date, home = home_team, away = away_team, home_goals = home_score, away_goals = away_score, competition = competition ) %&gt;% arrange(date) %&gt;% unique() abbrev &lt;- as_data_frame(table(c(team_data$home, team_data$away))) %&gt;% top_n(n = 1, wt = n) %&gt;% select(Var1) %&gt;% flatten_chr() if (nrow(team_data) &lt; 3) { ret_data &lt;- data_frame( club = y, abbrev = y, team_data = NA ) } else { if (abbrev == &quot;Sporting&quot;) { team_data$home[which(team_data$home == &quot;Sporting&quot;)] &lt;- y team_data$away[which(team_data$away == &quot;Sporting&quot;)] &lt;- y ret_data &lt;- data_frame( club = y, abbrev = y, team_data = list(team_data) ) } else { team_data &lt;- filter(team_data, home != &quot;Sporting&quot;, away != &quot;Sporting&quot;) ret_data &lt;- data_frame( club = y, abbrev = abbrev, team_data = list(team_data) ) } } return(ret_data) } "],
["diagnostic-output.html", "C Model Diagnostics Output C.1 Plot \\(\\hat{R}\\) values C.2 Plot effective sample size values C.3 NUTS diagnostics table C.4 PPMC score distribution plot C.5 PPMC margin of victory credible intervals C.6 Log loss comparison table", " C Model Diagnostics Output C.1 Plot \\(\\hat{R}\\) values as.data.frame(summary(gri_stanfit)[[1]]) %&gt;% mutate(Parameter = as.factor(gsub(&quot;\\\\[.*]&quot;, &quot;&quot;, rownames(.)))) %&gt;% ggplot(aes(x = Parameter, y = Rhat, color = Parameter)) + geom_jitter(height = 0, width = 0.4, show.legend = FALSE) + geom_hline(aes(yintercept = 1.1), linetype = &quot;dashed&quot;) + labs(y = expression(hat(italic(R)))) + theme_bw() C.2 Plot effective sample size values as.data.frame(summary(gri_stanfit)[[1]]) %&gt;% mutate(Parameter = as.factor(gsub(&quot;\\\\[.*]&quot;, &quot;&quot;, rownames(.)))) %&gt;% ggplot(aes(x = Parameter, y = n_eff, color = Parameter)) + geom_jitter(height = 0, width = 0.4, show.legend = FALSE) + expand_limits(y = 0) + labs(y = expression(hat(italic(R)))) + theme_bw() C.3 NUTS diagnostics table sp &lt;- get_sampler_params(gri_stanfit, inc_warmup = FALSE) E &lt;- as.matrix(sapply(sp, FUN = function(x) x[,&quot;energy__&quot;])) EBFMI &lt;- upars / apply(E, 2, var) mean_accept &lt;- sapply(sp, function(x) mean(x[, &quot;accept_stat__&quot;])) max_treedepth &lt;- sapply(sp, function(x) max(x[, &quot;treedepth__&quot;])) data_frame( Chain = paste0(&quot;Chain &quot;, 1:length(EBFMI)), `BFMI` = sprintf(&quot;%0.3f&quot;, EBFMI), `Mean Acceptance Rate` = sprintf(&quot;%0.3f&quot;, mean_accept), `Max Treedepth` = max_treedepth ) %&gt;% knitr::kable(caption = &quot;Diagnostic statistics for the NUTS algorithm.&quot;, align = &quot;c&quot;) C.4 PPMC score distribution plot rep_data %&gt;% filter(replication %in% 1:500) %&gt;% gather(key = team, value = score, home_score:away_score) %&gt;% mutate(team = factor(team, levels = c(&quot;home_score&quot;, &quot;away_score&quot;), labels = c(&quot;Home Score&quot;, &quot;Away Score&quot;))) %&gt;% ggplot(mapping = aes(x = score)) + facet_wrap(~ team, ncol = 2) + stat_density(aes(group = replication, color = &quot;Replications&quot;), geom = &quot;line&quot;, alpha = 0.2, bw = 0.5, position = &quot;identity&quot;) + stat_density( data = fit_data %&gt;% select(h_goals, a_goals) %&gt;% gather(key = team, value = score, h_goals:a_goals) %&gt;% mutate(team = factor(team, levels = c(&quot;h_goals&quot;, &quot;a_goals&quot;), labels = c(&quot;Home Score&quot;, &quot;Away Score&quot;))), aes(color = &quot;Observed&quot;), geom = &quot;line&quot;, alpha = 0.8, bw = 0.5 ) + scale_color_manual(values = c(&quot;Observed&quot; = &quot;red&quot;, &quot;Replications&quot; = &quot;black&quot;)) + theme_bw() + theme( legend.position = &quot;bottom&quot; ) + guides(color = guide_legend(title = NULL, override.aes = list(alpha = 1))) C.5 PPMC margin of victory credible intervals set.seed(32011) mov &lt;- rep_data %&gt;% mutate(mov = home_score - away_score) %&gt;% group_by(game) %&gt;% summarize( mean = mean(mov, na.rm = TRUE), pct025 = quantile(mov, probs = 0.025), pct25 = quantile(mov, probs = 0.25), pct50 = quantile(mov, probs = 0.5), pct75 = quantile(mov, probs = 0.75), pct975 = quantile(mov, probs = 0.975) ) %&gt;% mutate(obs_mov = fit_data$h_goals - fit_data$a_goals) %&gt;% mutate( cred50 = obs_mov &gt;= pct25 &amp; obs_mov &lt;= pct75, cred95 = obs_mov &gt;= pct025 &amp; obs_mov &lt;= pct975 ) mov %&gt;% filter(obs_mov &gt;= -5, obs_mov &lt;= 5, pct025 &gt;= -5, pct975 &lt;= 5) %&gt;% group_by(cred50, cred95) %&gt;% sample_n(3) %&gt;% arrange(desc(cred50), desc(cred95)) %&gt;% ungroup() %&gt;% mutate(game = factor(game) %&gt;% forcats::fct_inorder()) %&gt;% ggplot() + geom_linerange(aes(x = game, ymin = pct025, ymax = pct975, linetype = &quot;95% Credible Interval&quot;), color = &quot;#00BFC4&quot;) + geom_linerange(aes(x = game, ymin = pct25, ymax = pct75, linetype = &quot;50% Credible Interval&quot;), size = 1, color = &quot;#00BFC4&quot;) + geom_point(aes(x = game, y = obs_mov, color = &quot;Observed MOV&quot;), shape = 18, size = 4) + scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 1)) + scale_linetype_manual(values = c(&quot;95% Credible Interval&quot; = &quot;dashed&quot;, &quot;50% Credible Interval&quot; = &quot;solid&quot;)) + scale_color_manual(values = c(&quot;Observed MOV&quot; = &quot;#F8766D&quot;)) + labs(x = &quot;Game&quot;, y = &quot;Home Team Margin of Victory&quot;) + theme_bw() + theme( legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank() ) + guides( linetype = guide_legend(title = NULL, override.aes = list(size = 0.5)), color = guide_legend(title = NULL) ) C.6 Log loss comparison table data_frame( Model = c(&quot;Game Random Intercept&quot;, &quot;Data Average&quot;, &quot;Equal Probabilities&quot;, &quot;Home Win&quot;), `Log Loss` = c( avg_logloss, logloss(pred = matrix(data = c(mean(observations[,1]), mean(observations[,2]), mean(observations[,3])), nrow = nrow(outcomes), ncol = 3, byrow = TRUE), obs = observations), logloss(pred = matrix(data = 1/3, nrow = nrow(outcomes), ncol = 3), obs = observations), logloss(pred = matrix(data = c(1, 0, 0), nrow = nrow(outcomes), ncol = 3, byrow = TRUE), obs = observations) ) ) %&gt;% knitr::kable(caption = &quot;Log loss comparison to baseline models.&quot;, digits = 3, align = &quot;c&quot;) "],
["prediction-functions.html", "D Prediction Functions D.1 Predict individual games D.2 Plot multiple plots D.3 Predict leagues D.4 Predict Champions League", " D Prediction Functions D.1 Predict individual games predict_game &lt;- function(home, away, neutral = FALSE, visualize = TRUE, team_codes = team_counts, chains = model_params) { home_code &lt;- team_codes$code[which(team_codes$team == home)] away_code &lt;- team_codes$code[which(team_codes$team == away)] mu &lt;- chains$mu %&gt;% as.vector() eta &lt;- ifelse(neutral, 0, chains$eta %&gt;% as.vector()) home_off &lt;- chains$alpha[, home_code] home_def &lt;- chains$delta[, home_code] away_off &lt;- chains$alpha[, away_code] away_def &lt;- chains$delta[, away_code] sigma_g &lt;- chains$sigma_g %&gt;% as.vector() game_int &lt;- pmap_dbl(.l = list(sd = sigma_g), rnorm, n = 1, mean = 0) home_goals &lt;- exp(mu + eta + home_off + away_def + game_int) %&gt;% map_dbl(rpois, n = 1) away_goals &lt;- exp(mu + away_off + home_def + game_int) %&gt;% map_dbl(rpois, n = 1) outcomes &lt;- data_frame( club = c(home, away), expected_goals = c(mean(home_goals), mean(away_goals)), prob_win = c(length(which(home_goals &gt; away_goals)), length(which(away_goals &gt; home_goals))) / length(home_goals), prob_tie = rep(length(which(home_goals == away_goals)) / length(home_goals), 2), prob_loss = c(length(which(away_goals &gt; home_goals)), length(which(home_goals &gt; away_goals))) / length(home_goals) ) if (visualize) { heatmap &lt;- data_frame(home_goals, away_goals) %&gt;% group_by(home_goals, away_goals) %&gt;% summarize(probability = n() / nrow(.)) %&gt;% mutate(plot = &quot;Goal Distribution&quot;) histogram &lt;- data_frame(home_goals, away_goals) %&gt;% mutate(home_mov = home_goals - away_goals) %&gt;% select(home_mov) %&gt;% group_by(home_mov) %&gt;% summarize(probability = n() / nrow(.)) %&gt;% mutate(plot = &quot;Margin of Victory&quot;, winner = ifelse(home_mov &gt; 0, home, ifelse(home_mov &lt; 0, away, &quot;Tie&quot;))) %&gt;% mutate(winner = factor(winner, levels = c(home, &quot;Tie&quot;, away))) score_dist &lt;- ggplot(heatmap, aes(x = home_goals, y = away_goals)) + geom_tile(aes(fill = probability)) + scale_fill_gradient(name = &quot;Probability&quot;) + scale_x_continuous(breaks = seq(0, 100, 1)) + scale_y_continuous(breaks = seq(0, 100, 1)) + labs(x = paste0(home, &quot; Score&quot;), y = paste0(away, &quot; Score&quot;)) + theme_minimal() + theme( panel.grid.minor = element_blank(), legend.position = &quot;bottom&quot; ) mov_dist &lt;- ggplot(histogram, aes(x = home_mov, y = probability)) + geom_col(aes(fill = winner)) + scale_fill_brewer(type = &quot;qual&quot;, palette = 3, name = &quot;Winner&quot;) + scale_x_continuous(breaks = seq(-100, 100, 1)) + scale_y_continuous(breaks = seq(0, 1, 0.05), labels = scales::percent) + labs(x = paste0(home, &quot; Margin of Victory&quot;), y = &quot;Probability&quot;) + theme_minimal() + theme( panel.grid.minor.x = element_blank(), legend.position = &quot;bottom&quot; ) plot_list &lt;- list(score_dist, mov_dist) } if (visualize) { list( predictions = outcomes, plots = plot_list ) } else { list( predictions = outcomes ) } } D.2 Plot multiple plots multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots &lt;- length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots / cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots == 1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } D.3 Predict leagues predict_league &lt;- function(league, games = full_data, chains = model_params, team_codes = team_counts) { lookup &lt;- data_frame( league = c(&quot;Premier League&quot;, &quot;Bundesliga&quot;, &quot;La Liga&quot;, &quot;Ligue 1&quot;, &quot;Serie A&quot;), abbrev = c(&quot;Prem&quot;, &quot;Bund&quot;, &quot;La Liga&quot;, &quot;Ligue 1&quot;, &quot;Serie A&quot;), website = c(&quot;http://www.espnfc.us/english-premier-league/23/table&quot;, &quot;http://www.espnfc.us/german-bundesliga/10/table&quot;, &quot;http://www.espnfc.us/spanish-primera-division/15/table&quot;, &quot;http://www.espnfc.us/french-ligue-1/9/table&quot;, &quot;http://www.espnfc.us/italian-serie-a/12/table&quot;) ) abbrev &lt;- lookup$abbrev[which(lookup$league == league)] website &lt;- lookup$website[which(lookup$league == league)] safe_read_html &lt;- safely(read_html) cont &lt;- TRUE while(cont) { url_data &lt;- safe_read_html(website) if(is.null(url_data[[1]])) { closeAllConnections() Sys.sleep(5) } else { url_data &lt;- url_data[[1]] cont &lt;- FALSE } } league_table &lt;- url_data %&gt;% html_nodes(css = &quot;table&quot;) %&gt;% html_table() league_table &lt;- league_table[[1]] colnames(league_table) &lt;- as.character(league_table[1,]) colnames(league_table) &lt;- make.names(colnames(league_table), unique = TRUE) league_table &lt;- league_table[-1,] league_table &lt;- league_table %&gt;% select(club = TEAM, goals_for = `F`, goals_against = A, points = PTS) %&gt;% mutate(club = trimws(club, which = &quot;both&quot;), points = as.numeric(points), goals_for = as.numeric(goals_for), goals_against = as.numeric(goals_against)) club_names &lt;- league_table$club future_games &lt;- games %&gt;% filter(competition == abbrev, date &gt;= ymd(Sys.Date()), home %in% club_names, away %in% club_names) data_list &lt;- list( mu = chains$mu %&gt;% as.vector(), eta = chains$eta %&gt;% as.vector(), alpha = t(chains$alpha) %&gt;% as_data_frame() %&gt;% as.list(), delta = t(chains$delta) %&gt;% as_data_frame() %&gt;% as.list(), sigma_g = chains$sigma_g %&gt;% as.vector() ) league_sim &lt;- pmap_df(.l = data_list, .f = function(mu, eta, alpha, delta, sigma_g, sim_games, sim_league, team_codes) { for (g in seq_len(nrow(sim_games))) { home_code &lt;- team_codes$code[which(team_codes$team == sim_games$home[g])] away_code &lt;- team_codes$code[which(team_codes$team == sim_games$away[g])] home_off &lt;- alpha[home_code] home_def &lt;- delta[home_code] away_off &lt;- alpha[away_code] away_def &lt;- delta[away_code] game_int &lt;- rnorm(1, mean = 0, sd = sigma_g) sim_games$h_goals[g] &lt;- rpois(1, lambda = exp(mu + eta + home_off + away_def + game_int)) sim_games$a_goals[g] &lt;- rpois(1, lambda = exp(mu + away_off + home_def + game_int)) } sim_games &lt;- sim_games %&gt;% mutate( home_pts = ifelse(h_goals &gt; a_goals, 3, ifelse(h_goals &lt; a_goals, 0, 1)), away_pts = ifelse(h_goals &gt; a_goals, 0, ifelse(h_goals &lt; a_goals, 3, 1)) ) pts_total &lt;- bind_rows( select(sim_games, club = home, sim_pts = home_pts, sim_goals_for = h_goals, sim_goals_against = a_goals), select(sim_games, club = away, sim_pts = away_pts, sim_goals_for = a_goals, sim_goals_against = h_goals) ) %&gt;% group_by(club) %&gt;% summarize(sim_pts = sum(sim_pts), sim_goals_for = sum(sim_goals_for), sim_goals_against = sum(sim_goals_against)) final_league &lt;- left_join(sim_league, pts_total, by = &quot;club&quot;) %&gt;% mutate( tot_goals_for = goals_for + sim_goals_for, tot_goals_against = goals_against + sim_goals_against, goal_diff = tot_goals_for - tot_goals_against, tot_points = points + sim_pts ) %&gt;% arrange(desc(tot_points), desc(goal_diff), desc(tot_goals_for)) champ &lt;- final_league$club[1] final_league %&gt;% select(club, tot_points) %&gt;% arrange(club) %&gt;% spread(key = club, value = tot_points) %&gt;% mutate(Champion = champ) }, sim_games = future_games, sim_league = league_table, team_codes = team_codes) champions &lt;- data_frame(club = league_sim$Champion) %&gt;% group_by(club) %&gt;% summarize(champ_pct = n() / nrow(league_sim)) %&gt;% arrange(desc(champ_pct)) sim_results &lt;- league_sim %&gt;% select(-Champion) %&gt;% gather(key = club, value = points, everything()) %&gt;% group_by(club) %&gt;% summarize(sim_points = mean(points)) %&gt;% arrange(desc(sim_points)) %&gt;% left_join(champions, by = &quot;club&quot;) league_table %&gt;% left_join(sim_results, by = &quot;club&quot;) } D.4 Predict Champions League predict_ucl &lt;- function(matchups = matchups, games = full_data, chains = model_params, team_codes = team_counts) { all_clubs &lt;- flatten_chr(matchups) future_games &lt;- games %&gt;% filter(competition == &quot;UCL&quot;, home %in% all_clubs, away %in% all_clubs, date &gt;= ymd(Sys.Date()) - 30) data_list &lt;- list( mu = chains$mu %&gt;% as.vector(), eta = chains$eta %&gt;% as.vector(), alpha = t(chains$alpha) %&gt;% as_data_frame() %&gt;% as.list(), delta = t(chains$delta) %&gt;% as_data_frame() %&gt;% as.list(), sigma_g = chains$sigma_g %&gt;% as.vector() ) ucl_sim &lt;- pmap_df(.l = data_list, .f = function(mu, eta, alpha, delta, sigma_g, sim_games, sim_ucl, team_codes) { results &lt;- as_data_frame(matrix(data = NA, nrow = 1, ncol = 1)) colnames(results) &lt;- paste0(&quot;Round_&quot;, length(matchups)) repeat { for (g in seq_len(nrow(sim_games))) { if (sim_games$date[g] &lt; ymd(Sys.Date())) { next } home_code &lt;- team_codes$code[which(team_codes$team == sim_games$home[g])] away_code &lt;- team_codes$code[which(team_codes$team == sim_games$away[g])] home_off &lt;- alpha[home_code] home_def &lt;- delta[home_code] away_off &lt;- alpha[away_code] away_def &lt;- delta[away_code] game_int &lt;- rnorm(1, mean = 0, sd = sigma_g) if (length(matchups) == 1) { sim_games$h_goals[g] &lt;- rpois(1, lambda = exp(mu + home_off + away_def + game_int)) sim_games$a_goals[g] &lt;- rpois(1, lambda = exp(mu + away_off + home_def + game_int)) } else { sim_games$h_goals[g] &lt;- rpois(1, lambda = exp(mu + eta + home_off + away_def + game_int)) sim_games$a_goals[g] &lt;- rpois(1, lambda = exp(mu + away_off + home_def + game_int)) } } winners &lt;- map_chr(.x = matchups, .f = function(x, games) { games &lt;- filter(games, home %in% x, away %in% x) scores &lt;- bind_rows( select(games, club = home, h_goals) %&gt;% mutate(a_goals = 0), select(games, club = away, a_goals) %&gt;% mutate(h_goals = 0) ) %&gt;% group_by(club) %&gt;% summarize(h_goals = sum(h_goals), a_goals = sum(a_goals)) %&gt;% mutate(total_goals = h_goals + a_goals) %&gt;% arrange(desc(total_goals), desc(a_goals)) if (length(unique(scores$total_goals)) &gt; 1) { return(scores$club[1]) } else if (length(unique(scores$total_goals)) == 1 &amp;&amp; length(matchups) == 1) { return(sample(scores$club, 1)) } else if (length(unique(scores$total_goals)) == 1 &amp;&amp; length(unique(scores$a_goals)) &gt; 1) { return(scores$club[1]) } else { return(sample(scores$club, 1)) } }, games = sim_games) results[, paste0(&quot;Round_&quot;, length(winners))] &lt;- paste(sort(winners), collapse = &quot;,&quot;) if (length(winners) == 1) { break } if (length(winners) == 2) { matchups &lt;- list(c(winners[1], winners[2])) sim_games &lt;- data_frame(date = ymd(Sys.Date()), home = winners[1], away = winners[2], h_goals = NA, a_goals = NA, competition = &quot;UCL&quot;, home_game = 0) } else { new_match &lt;- data_frame(club = winners) %&gt;% mutate(matchup = sample(rep(1:(length(winners) / 2), 2), length(winners), replace = FALSE)) matchups &lt;- list_along(seq_len(length(winners) / 2)) new_games &lt;- list_along(seq_len(length(winners) / 2)) for (i in seq_along(matchups)) { clubs &lt;- new_match$club[which(new_match$matchup == i)] matchups[[i]] &lt;- clubs new_games[[i]] &lt;- data_frame(date = ymd(Sys.Date()), home = sort(clubs), away = rev(sort(clubs)), h_goals = NA, a_goals = NA, competition = &quot;UCL&quot;, home_game = 1) } sim_games &lt;- bind_rows(new_games) } } return(results) }, sim_games = future_games, sim_ucl = matchups, team_codes = team_codes) round_results &lt;- list_along(ucl_sim) names(round_results) &lt;- colnames(ucl_sim) for (i in seq_along(round_results)) { round_results[[i]] &lt;- paste(ucl_sim[[i]], collapse = &quot;,&quot;) %&gt;% strsplit(&quot;,&quot;) %&gt;% unlist() } total_sim &lt;- length(round_results$Round_1) final_results &lt;- data_frame(club = sort(flatten_chr(matchups)), Round_8 = 1, Round_4 = 1, Round_2 = 1, Round_1 = 1) for (i in seq_len(nrow(final_results))) { club &lt;- final_results$club[i] for (r in seq_along(round_results)) { final_results[[names(round_results)[r]]][i] &lt;- length(which(round_results[[r]] == club)) / total_sim } } final_results %&gt;% select(Club = club, Quarterfinals = Round_8, Semifinals = Round_4, Final = Round_2, Champion = Round_1) %&gt;% arrange(desc(Champion)) } "],
["references.html", "References", " References "]
]
